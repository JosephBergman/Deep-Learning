<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-10-04T01:45:19+09:00</updated><id>http://localhost:4000/</id><title type="html">Deep Learning</title><subtitle>Project based blog on deep learning</subtitle><author><name>Joseph Bergman</name><email>Joseph.T.Bergman@gmail.com</email></author><entry><title type="html">Neural Network Basics</title><link href="http://localhost:4000/2018-10-02/neural-network-basics" rel="alternate" type="text/html" title="Neural Network Basics" /><published>2018-10-02T00:00:00+09:00</published><updated>2018-10-02T00:00:00+09:00</updated><id>http://localhost:4000/2018-10-02/neural-network-basics</id><content type="html" xml:base="http://localhost:4000/2018-10-02/neural-network-basics">&lt;p&gt;Throughout the next few posts we will answer the following three questions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What is a neural network?&lt;/li&gt;
  &lt;li&gt;What does a neural network compute?&lt;/li&gt;
  &lt;li&gt;How can we teach a neural network to “learn” from examples?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This post will focus on the first question.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;what-is-a-neural-network&quot;&gt;What is a Neural Network?&lt;/h2&gt;
&lt;p&gt;An Artificial Neural Network (ANN), or simply “neural network”, is a biologically-inspired programming paradigm which enables a program to learn from data without being explicitly programmed. For example, we can show a neural network thousands of cat/not-cat photos and the neural network can learn on its own to predict whether or not a given photo contains a cat.&lt;/p&gt;

&lt;p&gt;Similar to the human brain, a neural network is composed of many smaller and simpler units known as “(artificial) neurons”. Individually, each neuron performs a relatively simple computation, but by feeding the output of one neuron into other neurons we can compute progressively more complex functions. Neural networks currently provide the best solutions to a number of problems in image recognition, speech recognition, and natural language processing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post01/nn-example.svg&quot; alt=&quot;Example of a Neural Network&quot; width=&quot;200px&quot; /&gt;
&lt;em&gt;An example of a 5-layer neural network&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;More precisely, a neural network is a map from some input space to a range of output values. The input space and output space will vary depending on the problem. For instance, the input could be a photo of a handwritten digit and the output would be some digit between 0 and 9. In a more complex case, the input could be some photo and the output could be a caption for that photo. What makes neural networks so powerful is that they can learn the mapping between the input and output just by looking at examples.&lt;/p&gt;

&lt;p&gt;Before we can understand how a neural network learns, we need to understand what a neural network does to map a given input to an output. And before we can understand how a neural network maps an input to an output, we need to understand what a single neuron does. We will begin by looking at the simplest type of artificial neuron – the perceptron.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;the-perceptron&quot;&gt;The Perceptron&lt;/h2&gt;
&lt;p&gt;A perceptron takes an arbitrary number of inputs &lt;script type=&quot;math/tex&quot;&gt;x_1, x_2, \dots&lt;/script&gt; and generates a single binary output &lt;script type=&quot;math/tex&quot;&gt;a \in \{0, 1\}&lt;/script&gt;. Each input &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; has a corresponding weight &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt; which expresses how important that input is to the output. The perceptron also has a bias &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; which is a single real number and expresses how willing the perceptron is to “fire” (produce an output of 1).&lt;/p&gt;

&lt;p&gt;The perceptron computes its output in two steps. First, we multiply all the inputs by their corresponding weights and add the bias. This can be simplified by treating &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt; as column vectors and taking their dot product:
&lt;script type=&quot;math/tex&quot;&gt;z = w^T x + b&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Then, we compute the output according to the following function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
g(z) =
\begin{cases}
1, &amp; \text{if $z &gt; 0$} \\
0, &amp; \text{if $z \leq 0$}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;The function &lt;script type=&quot;math/tex&quot;&gt;g(z)&lt;/script&gt; is an example of an “activation function”, and will be the only thing that changes in our other neurons.&lt;/p&gt;

&lt;h3 id=&quot;example-of-a-perceptron&quot;&gt;Example of a Perceptron&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/post01/perceptron-example.png&quot; alt=&quot;Example of a Perceptron&quot; /&gt;
&lt;em&gt;(&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap1.html&quot;&gt;source&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The perceptron above takes two inputs &lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_2&lt;/script&gt; which have corresponding weights &lt;script type=&quot;math/tex&quot;&gt;w_1 = -2&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w_2 = -2&lt;/script&gt; as shown. The perceptron has a bias of &lt;script type=&quot;math/tex&quot;&gt;b = 3&lt;/script&gt;. The value of the output depends on the specific values of the inputs. For example, suppose &lt;script type=&quot;math/tex&quot;&gt;x_1 = 1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_2 = 0&lt;/script&gt;. Then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x =
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix}

\qquad

w =
\begin{bmatrix}
-2 \\
-2 \\
\end{bmatrix}

\qquad
b = 3&lt;/script&gt;

&lt;p&gt;It follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
z &amp; = w^T x + b \\
  &amp; = (-2\cdot 1 + -2\cdot 0) + 3 \\
  &amp; = (-2) + 3 \\
  &amp; = 1
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since this is positive our activation function gives us the following&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\text{output} &amp; = g(z) \\
  &amp; = g(1) \\
  &amp; = 1 \quad \text{since $z &gt; 0$}
\end{align} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;problems-with-the-perceptron&quot;&gt;Problems with the Perceptron&lt;/h3&gt;
&lt;p&gt;Ultimately we want to devise algorithms which allow our neural network to learn the weights &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; and biases &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; on its own. To be able to learn, we want a small change to the weights or bias to cause a small change in the corresponding output. If we can achieve this, we can gradually update our weights and biases until our outputs are very accurate. Unfortunately, perceptrons don’t have this feature.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post01/perceptron-activation.png&quot; alt=&quot;Perceptron Activation Function&quot; height=&quot;200&quot; /&gt;
&lt;em&gt;Perceptron Activation Function (&lt;a href=&quot;https://www.codeproject.com/Articles/1216170/Common-Neural-Network-Activation-Functions&quot;&gt;Source&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Looking at the perceptron activation function, we can see that a small change in the weights or biases can have one of two effects:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The output won’t change at all&lt;/li&gt;
  &lt;li&gt;The output will change completely&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is because the perceptron activation function is (1) not continuous and (2) has a derivative of zero when it is differentiable. We want activation functions that are both continuous and differentiable. In the next section, we will look at four different activation functions that we can use with neurons in our neural network.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;activation-functions&quot;&gt;Activation Functions&lt;/h2&gt;
&lt;p&gt;In this section we will look at four different activation functions, &lt;script type=&quot;math/tex&quot;&gt;g(\cdot)&lt;/script&gt;:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The sigmoid function: &lt;script type=&quot;math/tex&quot;&gt;\sigma(z)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;The hyperbolic tangent function: &lt;script type=&quot;math/tex&quot;&gt;\tanh(z)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;The ReLU function: &lt;script type=&quot;math/tex&quot;&gt;\text{ReLU}(z)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;The Leaky ReLU function: &lt;script type=&quot;math/tex&quot;&gt;\text{LeakyReLU}(z)&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;the-sigmoid-function&quot;&gt;The Sigmoid Function&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/post01/sigmoid-activation.png&quot; alt=&quot;Sigmoid Activation Function&quot; height=&quot;200&quot; /&gt;
&lt;em&gt;Sigmoid Activation Function (&lt;a href=&quot;https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e&quot;&gt;Source&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The sigmoid function has the following formula&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma(z) = \frac{1}{1 + e^(-z)}&lt;/script&gt;

&lt;p&gt;The sigmoid function has the following derivate (&lt;a href=&quot;http://kawahara.ca/how-to-compute-the-derivative-of-a-sigmoid-function-fully-worked-example/&quot;&gt;derivation&lt;/a&gt;)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma'(z) = \sigma(z) (1 - \sigma(z))&lt;/script&gt;

&lt;p&gt;The sigmoid function is both continuous and differentiable as desired. Another useful property of the sigmoid function is that the output of the sigmoid function is always between 0 and 1, so we can interpret the output of the function as a probability.&lt;/p&gt;

&lt;h3 id=&quot;the-hyperbolic-tangent-function&quot;&gt;The Hyperbolic Tangent Function&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/post01/tanh-activation.jpg&quot; alt=&quot;Hyperbolic Tangent Activation Function&quot; height=&quot;200&quot; /&gt;
&lt;em&gt;Hyperbolic Tangent Activation Function (&lt;a href=&quot;http://www.20sim.com/webhelp/language_reference_functions_tanh.php&quot;&gt;Source&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The hyperbolic tangent function has the following formula&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}&lt;/script&gt;

&lt;p&gt;The hyperbolic tangent function has the following derivate&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tanh'(z) = 1 - (tanh^2(z))^2&lt;/script&gt;

&lt;p&gt;The hyperbolic tangent function is continuous and differentiable as desired. Another useful property of the hyperbolic tangent function is that the output is between -1 and 1 and centered around 0 (see graph above). The result is that the hyperbolic function has a way of “centering” our data about 0 which tends to cause our neural networks to learn faster.&lt;/p&gt;

&lt;p&gt;In course one of the &lt;a href=&quot;https://www.coursera.org/specializations/deep-learning&quot;&gt;Deep Learning Specialization&lt;/a&gt; Professor Ng says that hyperbolic tangent is almost always better than the sigmoid function, except for possibly in the output layer where we may want to use the sigmoid function to represent a probability.&lt;/p&gt;

&lt;h3 id=&quot;the-rectified-linear-unit-relu&quot;&gt;The Rectified Linear Unit (ReLU)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/post01/relu-activation.png&quot; alt=&quot;ReLU Activation Function&quot; height=&quot;200&quot; /&gt;
&lt;em&gt;ReLU Activation Function (&lt;a href=&quot;https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/&quot;&gt;Source&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The ReLU has a very simple formula&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{ReLU}(z) = \max(0, z)&lt;/script&gt;

&lt;p&gt;The derivative of the ReLU is as follows&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\text{ReLU}'(z) =
\begin{cases}
1, &amp; \text{if $z &gt; 0$} \\
0, &amp; \text{if $z &lt; 0$} \\
\text{DNE}, &amp; \text{if z = 0, in practice use 0 or 1}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;The ReLU function is continuous and differentiable everywhere except &lt;script type=&quot;math/tex&quot;&gt;z = 0&lt;/script&gt;. In practice, this does not matter and you can set the derivative at &lt;script type=&quot;math/tex&quot;&gt;z = 0&lt;/script&gt; to be either 1 or 0. The ReLU function is currently the most used activation function.&lt;/p&gt;

&lt;p&gt;If you look at the sigmoid and hyperbolic tangent functions, you will notice that for very large or very small values of &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; the derivative is nearly 0. This will slow down learning, so the ReLU is preferred. Of course, the derivative of the ReLU is 0 for all &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
z &lt; 0 %]]&gt;&lt;/script&gt; which leads to our final activation function.&lt;/p&gt;

&lt;h3 id=&quot;the-leaky-relu&quot;&gt;The Leaky ReLU&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/post01/leaky-relu-activation.png&quot; alt=&quot;Leaky ReLU Activation Function&quot; height=&quot;200&quot; /&gt;
&lt;em&gt;Leaky ReLU Activation Function (&lt;a href=&quot;https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/&quot;&gt;Source&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The Leaky ReLU also has a simple formula&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{LeakyReLU}(z) = \max(0.01z, z)&lt;/script&gt;

&lt;p&gt;The derivative of the Leaky ReLU is as follows&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\text{LeakyReLU}'(z) =
\begin{cases}
1, &amp; \text{if $z &gt; 0$} \\
0.01, &amp; \text{if $z &lt; 0$} \\
\text{DNE}, &amp; \text{if z = 0, in practice use 0.01 or 1}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;The Leaky ReLU function is extremely similar to the ReLU function, but has a non-zero derivative for &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
z &lt; 0 %]]&gt;&lt;/script&gt;. The Leaky ReLU actually works quite well in practice, but it is rarely used compared to the standard ReLU function.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;from-neurons-to-neural-networks&quot;&gt;From Neurons to Neural Networks&lt;/h2&gt;
&lt;p&gt;Now that we have established what neurons are, how they compute their output, and what activation functions they can use, we can start to discuss neural networks. In this section, we will look at some neural network terminology, and we will compute the output of a neural network by hand. In the next post, we will establish some more mathematical notation to simplify this process.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post01/nn-terminology-example.svg&quot; alt=&quot;Neural Network Example&quot; height=&quot;250&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;terminology&quot;&gt;Terminology&lt;/h3&gt;
&lt;p&gt;Above is a diagram for a simple 2-layer neural network. It may look like there are three layers, but the layer all the way on the left is the “Input Layer” and we don’t count it. Thus the neural network shown above has 2-layers – one hidden layer and one output layer. Since our end-user only cares about the input and the output, we call all the layers in between “hidden layers”. An L-layer neural network will have L-1 hidden layers.&lt;/p&gt;

&lt;p&gt;The nodes in the input layer, labeled &lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_2&lt;/script&gt; are not neurons, they are just real valued inputs. The hidden layer, however, contains three neurons and the output layer contains a single neuron.&lt;/p&gt;

&lt;p&gt;Each neuron in the hidden layer takes two inputs, &lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_2&lt;/script&gt; and produces a single output. The neuron in the output layer takes three inputs, lets call them &lt;script type=&quot;math/tex&quot;&gt;h_1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;h_2&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;h_3&lt;/script&gt;, and produces a single output &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;. Every edge in the neural network will have a weight associated with it, so this network has 9 weights in total.&lt;/p&gt;

&lt;p&gt;To see how a neural network computes its output, let’s give these variables some values and work through an example by hand. As mentioned, we will simplify the math and write some code to do this for us in the next post.&lt;/p&gt;

&lt;h3 id=&quot;assigning-values-to-our-network&quot;&gt;Assigning Values to our Network&lt;/h3&gt;
&lt;p&gt;Let’s suppose the inputs have the following values&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_1 = 1 \qquad x_2 = 2&lt;/script&gt;

&lt;p&gt;Then we can create a column vector containing our inputs&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x =
\begin{bmatrix}
1 \\
2 \\
\end{bmatrix}&lt;/script&gt;

&lt;p&gt;Let’s give our hidden neurons some weights. Since each neuron takes two inputs, each neuron will need to have two weights. Suppose that &lt;script type=&quot;math/tex&quot;&gt;h_i&lt;/script&gt; has weights &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt; as follows&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_1 =
\begin{bmatrix}
0.25 \\
-.25 \\
\end{bmatrix}

\qquad

w_2 =
\begin{bmatrix}
-0.5 \\
0.5 \\
\end{bmatrix}

\qquad

w_3 =
\begin{bmatrix}
-1 \\
1 \\
\end{bmatrix}&lt;/script&gt;

&lt;p&gt;Our output neuron has three inputs, so it is going to need three weights. Let’s suppose that our output neuron &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; has the following weights&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_y =
\begin{bmatrix}
-1 \\
0 \\
1 \\
\end{bmatrix}&lt;/script&gt;

&lt;p&gt;Finally, let’s assume that the bias of each neuron is 0, and let’s assume that the hidden neurons use ReLU activation and that the output neuron uses sigmoid activation.&lt;/p&gt;

&lt;h3 id=&quot;computing-the-output&quot;&gt;Computing the Output&lt;/h3&gt;
&lt;p&gt;With all of the values given above we have the following network&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/post01/nn-example-labeled.svg&quot; alt=&quot;Labeled Neural Network Example&quot; height=&quot;250&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In order to compute our final output, we first have to compute the output of each hidden neuron. We will essentially pass our input values “forward” through the network until we reach the output layer. The computations are below.&lt;/p&gt;

&lt;p&gt;For our first hidden neuron&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
z_{h1} &amp; = w^T x + b \\
  &amp; = (0.25\cdot 1 + -0.25\cdot 2) + 0 \\
  &amp; = -0.25 \\
a_{h1} &amp; = \text{ReLU}(-0.25) \\
       &amp; = 0
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;For our second hidden neuron&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
z_{h2} &amp; = w^T x + b \\
  &amp; = (-0.5\cdot 1 + 0.5\cdot 2) + 0 \\
  &amp; = 0.5 \\
a_{h1} &amp; = \text{ReLU}(0.5) \\
       &amp; = 0.5
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;For our third hidden neuron&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
z_{h2} &amp; = w^T x + b \\
  &amp; = (-1\cdot 1 + 1\cdot 2) + 0 \\
  &amp; = 1.0 \\
a_{h1} &amp; = \text{ReLU}(1.0) \\
       &amp; = 1.0
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can combine the hidden layer outputs into an input vector.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h =
\begin{bmatrix}
0 \\
0.5 \\
1.0 \\
\end{bmatrix}&lt;/script&gt;

&lt;p&gt;Our output layer is a single neuron with sigmoid activation, so we get our final output&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
z_{y} &amp; = w^T h + b \\
  &amp; = (-1\cdot 0 + 0\cdot 0.5 + 1\cdot 1) + 0 \\
  &amp; = 1.0 \\
a_{y} &amp; = \sigma(1.0) \\
       &amp; \approx 0.73
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;You can check the sigmoid function computation &lt;a href=&quot;https://www.wolframalpha.com/input/?i=sigmoid(1)&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;review&quot;&gt;Review&lt;/h2&gt;
&lt;p&gt;Right now, it may not clear how a neural network learns or why they’re useful. For now, focus on the following takeaways.&lt;/p&gt;

&lt;h3 id=&quot;what-is-a-neural-network-1&quot;&gt;What is a neural network?&lt;/h3&gt;
&lt;p&gt;A neural network is a biologically-inspired programming paradigm that learns from examples without being explicitly programmed.&lt;/p&gt;

&lt;h3 id=&quot;what-is-a-neural-network-made-of&quot;&gt;What is a neural network made of?&lt;/h3&gt;
&lt;p&gt;A neural network is composed of many smaller units called neurons. Neurons weight their inputs, add a bias, and apply an activation function to create a single output. The output of one neuron can be fed into many other neurons allowing the neural network to learn complex mappings.&lt;/p&gt;

&lt;h3 id=&quot;what-does-a-neural-network-do&quot;&gt;What does a neural network do?&lt;/h3&gt;
&lt;p&gt;A neural network learns to map inputs to their desired outputs. For instance, given a photo output whether or not there is a cat in it. Another example could be, given some audio recording output a transcript in English.&lt;/p&gt;

&lt;h3 id=&quot;how-does-a-neural-network-map-an-input-to-an-output&quot;&gt;How does a neural network map an input to an output?&lt;/h3&gt;
&lt;p&gt;As we saw in the final example, a neural network passes its inputs through each layer of the network until they reach the output layer. The final output of a neural network will be determined by the weights, biases, and activations used within the network. In the next post, we will look at how we can efficiently map an input to its corresponding output.&lt;/p&gt;</content><author><name>Joseph Bergman</name><email>Joseph.T.Bergman@gmail.com</email></author><summary type="html">Throughout the next few posts we will answer the following three questions:</summary></entry></feed>