{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Deep Dream with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Introduction\n",
    "In this post, we are going to implement the Deep Dream algorithm in Keras.\n",
    "\n",
    "> DeepDream is a computer vision program created by Google engineer Alexander Mordvintsev which uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dream-like hallucinogenic appearance in the deliberately over-processed images ([source](https://en.wikipedia.org/wiki/DeepDream))\n",
    "\n",
    "Sound complicated? Whether you know it or not, you've probably seen deep dream before.\n",
    "\n",
    "![Deep Dream Image 2](./images/deep_dream_2.jpg)\n",
    "\n",
    "Deep dream uses the representations learned by convolutional neural networks to to modify the input image and introduce trippy, dream-like artifacts. The idea is simple, at each iteration of training, modify the input image to increase the output of a given layer. If that layer was trained to recognize dogs, then your image will slowly start to contain dog-like patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Choosing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepDream can work with any trained convnet, but different convnets will have learned different features, so they will give different results. The original DeepDream implementation used an Inception model, so we will use InceptionV3 which is included with Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import InceptionV3\n",
    "from keras import backend as K \n",
    "\n",
    "# Disable training \n",
    "K.set_learning_phase(0)\n",
    "\n",
    "# Load the pretrained model, we don't need the output layers\n",
    "model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Defining a Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update our image, we will use Keras to maximize the average output of a number of layers. The layers we choose to maximize will change the results of the algorithms. If we use earlier layers, which recognize edges and textures, then we will introduce simple patterns into our image. If we use later layers, which recognize birds and dogs, we will introduce more complex patterns into our image. We will combine the results of several different layers to make our model a bit more diverse. Note that these layer names can be discovered by using `model.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_coefficients = {\n",
    "    'mixed1': 1.0, \n",
    "    'mixed2': 1.0,\n",
    "    'mixed3': 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined how much to weight each layer, we will define the loss function. The loss function is simply the sum of the L2 norms of the layers we selected above. The L2 norm of each layer is scaled by the coefficient we selected above. Note that we are keeping all of the channels in the output, but we are removing the borders from each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary: layer name => layer\n",
    "layers = {layer.name: layer for layer in model.layers}\n",
    "\n",
    "# Store the loss \n",
    "loss = K.variable(0.)\n",
    "\n",
    "# Iterate through our chosen layers and add to loss function \n",
    "for layer_name, layer_coeff in layer_coefficients.items():\n",
    "    activation = layers[layer_name].output\n",
    "    scaling = K.prod(K.cast(K.shape(activation), 'float32'))\n",
    "    loss = loss + layer_coeff * K.sum(K.square(activation[:, :, :, :])) / scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Gradient Ascent Process\n",
    "The gradient ascent process is where we actually modify the input image. Typically, when training a neural network, we use _gradient descent_. This minimizes a loss function with respect to the weights and biases in our network. For DeepDream, we are going to use _gradient descent_ to maximize our loss function with respect to the input images. This will cause the input image – the 'dream' – to form patterns that our chosen layers recognize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold the input image \n",
    "dream = model.input\n",
    "\n",
    "# Compute the gradients with regard to the loss and normalize \n",
    "grads = K.gradients(loss, dream)[0]\n",
    "grads = grads / K.maximum(K.mean(K.abs(grads)), 1e-7)\n",
    "\n",
    "# Tells Keras we want the loss and gradient for a given image \n",
    "loss_and_grads = K.function([dream], [loss, grads])\n",
    "\n",
    "def get_loss_and_gradients(image):\n",
    "    output = loss_and_grads([image])\n",
    "    loss = output[0]\n",
    "    gradient = output[1]\n",
    "    return loss, gradient\n",
    "\n",
    "# Run Gradient Ascent \n",
    "def gradient_ascent(image, iterations, step, max_loss=None, verbose=True):\n",
    "    for i in range(iterations):\n",
    "        loss, gradient = get_loss_and_gradients(image)\n",
    "        if max_loss is not None and loss > max_loss:\n",
    "            break \n",
    "        if verbose:\n",
    "            print('Loss at iteration', i, 'is', loss)\n",
    "        image = image + step * gradient # the actual update\n",
    "    return image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The DeepDream Algorithm\n",
    "The actual DeepDream algorithm runs on the image at a number of differnt scales (called octaves). We apply gradient ascent to the image at the initial scale, scale it up by the determined amount (1.4 = 40%), then we reinsert lost detail (due to resizing) and repeat the process. For example, the default parameters shown below will run the image at 3 different sizes going from smallest to largest. After running gradient ascent, the image size will be increased by 40%, and the process will repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from helpers.helpers import *\n",
    "\n",
    "def dream(image_path, step=0.01, num_octaves=3, octave_scale=1.4, iterations=20, max_loss=10, verbose=True):\n",
    "    \"\"\"Implements the deep dream algorithm.\"\"\"\n",
    "    # Load the image\n",
    "    img = preprocess_image(image_path)\n",
    "    \n",
    "    # Define what scales to run gradient descent\n",
    "    image_shapes = image_octaves(img, octave_scale, num_octaves)\n",
    "    \n",
    "    # Create image copies\n",
    "    original_img = np.copy(img)\n",
    "    shrunk_original_img = resize_image(img, image_shapes[0])\n",
    "    \n",
    "    # Process the image at each scale\n",
    "    for shape in image_shapes:\n",
    "        print('Processing image shape', shape)\n",
    "        img = resize_image(img, shape)\n",
    "        img = gradient_ascent(img, iterations=iterations, step=step, \n",
    "                              max_loss=max_loss, verbose=verbose)\n",
    "        \n",
    "        # Insert 'detail' into resized image\n",
    "        upscaled_shrunk_original_img = resize_image(shrunk_original_img, shape)\n",
    "        same_size_original = resize_image(original_img, shape)\n",
    "        lost_detail = same_size_original - upscaled_shrunk_original_img\n",
    "        img += lost_detail \n",
    "        shrunk_original_img = resize_image(original_img, shape)\n",
    "        save_image(img, filename='dream_at_scale_' + str(shape) + '.png')\n",
    "    \n",
    "    ext = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    filename = 'final_dream_' + ext + '.png'\n",
    "    save_image(img, filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use this function to generate dreams. Feel free to modify the parameters or supply your own images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image shape (416, 416)\n",
      "Processing image shape (500, 500)\n",
      "Processing image shape (600, 600)\n"
     ]
    }
   ],
   "source": [
    "dream('./inputs/shiba.jpg', step=0.01, num_octaves=3, iterations=25, octave_scale=1.2, max_loss=None, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Some Examples\n",
    "\n",
    "### Original Image\n",
    "![Original Shiba Image](./inputs/shiba.jpg)\n",
    "\n",
    "### Example 1\n",
    "```\n",
    "layer_coefficients = {\n",
    "    'conv2d_1': 1.0, \n",
    "}\n",
    "dream('./inputs/shiba.jpg', step=0.01, num_octaves=1, iterations=20, max_loss=None, verbose=True)\n",
    "```\n",
    "![Example 1](./outputs/final_dream_shiba_conv2d_1.png)\n",
    "\n",
    "\n",
    "### Example 2\n",
    "```\n",
    "layer_coefficients = {\n",
    "    'conv2d_4': 1.0, \n",
    "}\n",
    "dream('./inputs/shiba.jpg', step=0.01, num_octaves=1, iterations=20, max_loss=None, verbose=True)\n",
    "```\n",
    "![Example 2](./outputs/final_dream_shiba_conv2d_4.png)\n",
    "\n",
    "\n",
    "### Example 3\n",
    "```\n",
    "layer_coefficients = {\n",
    "    'mixed1': 1.0, \n",
    "}\n",
    "dream('./inputs/shiba.jpg', step=0.01, num_octaves=1, iterations=20, max_loss=None, verbose=True)\n",
    "```\n",
    "![Example 3](./outputs/final_dream_shiba_mixed1.png)\n",
    "\n",
    "\n",
    "### Example 4\n",
    "```\n",
    "layer_coefficients = {\n",
    "    'mixed10': 1.0, \n",
    "}\n",
    "dream('./inputs/shiba.jpg', step=0.01, num_octaves=1, iterations=20, max_loss=None, verbose=True)\n",
    "```\n",
    "![Example 4](./outputs/final_dream_shiba_mixed10.png)\n",
    "\n",
    "\n",
    "### Example 5\n",
    "```\n",
    "layer_coefficients = {\n",
    "    'conv2d_1': 1.0, \n",
    "    'conv2d_4': 1.0,\n",
    "    'conv2d_12': 1.0,\n",
    "}\n",
    "dream('./inputs/shiba.jpg', step=0.01, num_octaves=3, iterations=20, max_loss=100, verbose=True)\n",
    "```\n",
    "![Example 5](./outputs/final_dream_shiba_3_layers_3_octaves.png)\n",
    "\n",
    "\n",
    "### Example 6\n",
    "```\n",
    "layer_coefficients = {\n",
    "    'mixed1': 1.0, \n",
    "    'mixed2': 1.0,\n",
    "    'mixed3': 1.0,\n",
    "}\n",
    "dream('./inputs/shiba.jpg', step=0.01, num_octaves=1, iterations=50, max_loss=None, verbose=False)\n",
    "```\n",
    "![Example 6](./outputs/final_dream_shiba.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## References\n",
    "The photo of the Shiba can be found [here](https://t2.ea.ltmcdn.com/en/images/3/5/1/img_shedding_of_shiba_inus_153_600.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
