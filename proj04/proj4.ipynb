{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Neural Network from Scratch with Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Our Network\n",
    "We will be creating a neural network from scratch using `numpy` and `autograd`. This post is intended to be a fun experiment and get us a bit closer to underlying implementation than we usually are in Keras. In fact, when I first started this project I thought it would be cool if we did everything from scratch. I realized, however, that it is incredibly time consuming and it is far more important to be able to iterate quickly. \n",
    "\n",
    "There is one other thing that ~~sucks~~ is unpleasant when creating neural networks from scratch â€“ backpropagation. In my opinion, backpropagation is the trickiest part to get right when implementing a neural network. Fortunately, we can use gradient checking to get an idea whether or not our implementation is correct, but that means we have to implement gradient checking. Today, instead of implementing backpropagation from scratch, we are going to use [`autograd`](https://github.com/HIPS/autograd) which is a neat little tool that can automatically compute derivatives of native Python and Numpy code. \n",
    "\n",
    "To install autograd, run the following\n",
    "```bash\n",
    "pip install autograd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading the Data\n",
    "We are going to use the Keras MNIST data. The following steps should look familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n",
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()\n",
    "\n",
    "# Reshape\n",
    "train_data = train_data.reshape(60000, 28*28)\n",
    "test_data = test_data.reshape(10000, 28*28)\n",
    "\n",
    "# Scale\n",
    "train_data = train_data.astype('float32') / 255\n",
    "test_data = test_data.astype('float32') / 255\n",
    "\n",
    "# One-hot Encode\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Check Shapes\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "# Train / Val / Test\n",
    "x_val = train_data[:10000]\n",
    "x_train = train_data[10000:]\n",
    "y_val = train_labels[:10000]\n",
    "y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np \n",
    "from autograd.scipy.misc import logsumexp\n",
    "from autograd import grad \n",
    "from autograd.misc.optimizers import adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_parameters(layer_sizes, scale):\n",
    "    \"\"\"Returns a list of (weights, bias) tuples representing NN.\"\"\"\n",
    "    return [(scale * np.random.randn(m, n), scale * np.zeros(n))\n",
    "            for m, n in zip(layer_sizes[:-1], layer_sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(params, inputs): \n",
    "    \"\"\"Performs forward propagation for the given inputs.\"\"\"\n",
    "    for W, b in params:\n",
    "        outputs = np.dot(inputs, W) + b\n",
    "        inputs = np.tanh(outputs)\n",
    "    return outputs - logsumexp(outputs, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(params, inputs):\n",
    "    \"\"\"Return the predicted classes for the given inputs.\"\"\"\n",
    "    outputs = feed_forward(params, inputs)\n",
    "    return np.argmax(outputs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(params, inputs, targets):\n",
    "    \"\"\"Return the network's accuracy for the given inputs and targets.\"\"\"\n",
    "    targets = np.argmax(targets, axis=1)\n",
    "    predictions = predict(params, inputs)\n",
    "    print(\"Assert\")\n",
    "    assert(targets.shape == predictions.shape)\n",
    "    return np.mean(predictions == targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_posterior(params, inputs, targets):\n",
    "    return np.sum(feed_forward(parameters, inputs) * targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "layer_sizes = [784, 128, 128, 10]\n",
    "\n",
    "# Training Parameters\n",
    "batch_size = 128\n",
    "scale = 0.1\n",
    "epochs = 20\n",
    "lr = 0.001\n",
    "\n",
    "parameters = init_parameters(layer_sizes, scale)\n",
    "\n",
    "# Minibatches\n",
    "num_batches = int(np.ceil(len(x_train) / batch_size))\n",
    "def batch_indices(iter):\n",
    "    idx = iter % num_batches\n",
    "    return slice(idx * batch_size, (idx+1) * batch_size)\n",
    "\n",
    "# Define training objective \n",
    "def objective(params, iter):\n",
    "    idx = batch_indices(iter)\n",
    "    return - log_posterior(params, x_train[idx], y_train[idx])\n",
    "\n",
    "# Get gradient using autograd \n",
    "objective_grad = grad(objective)\n",
    "\n",
    "def print_perf(params, iter, gradient):\n",
    "    if iter % num_batches == 0:\n",
    "        train_acc = accuracy(params, x_train, y_train)\n",
    "        val_acc = accuracy(params, x_val, y_val)\n",
    "        print(\"{:15}|{:20}|{:20}\".format(iter//num_batches, train_acc, val_acc))\n",
    "\n",
    "# Optimize parameters \n",
    "optimize_params = adam(objective_grad, parameters, step_size=lr, num_iters=epochs * num_batches, callback=print_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
