{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Introduction\n",
    "In this post, we're going to implement a fully-connected neural network from scratch and use it to perform sentiment analysis on the IMDB movie reviews dataset. That is â€“ we'll create a network to tell us if a movie review is positive or negative. The IMDB dataset is included with Keras, so let's dive right in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data like usual. The `num_words=10000` says that we only want to keep that 10,000 most frequent words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the reviews are stored as integer sequences where each integer represents a given word. The following function can convert an integer sequence back into the original review. The `word_index` maps each word to its integer, and by reversing that index, we can map each integer to its word. The '?' represents a word that was not in the top 10,000 words, so we don't know what it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review\n",
      "~~~~~~\n",
      "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "\n",
      "Sentiment\n",
      "~~~~~~~~~\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def sequence_to_text(sequence):\n",
    "    \"\"\"Converts an integer sequence into the actual review text.\"\"\"\n",
    "    word_index = imdb.get_word_index()\n",
    "    reverse_index = {value: key for (key, value) in word_index.items()}\n",
    "    return ' '.join([reverse_index.get(i - 3, '?') for i in sequence])\n",
    "\n",
    "print(\"Review\")\n",
    "print(\"~~~~~~\")\n",
    "print(sequence_to_text(train_data[0]), end=\"\\n\\n\")\n",
    "\n",
    "print(\"Sentiment\")\n",
    "print(\"~~~~~~~~~\")\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to prepare our data to be fed into a neural network. We'll do this by creating a binary vector to represent each review. Each vector will be be `(10000,1)` and a 1 at index $i$ will indicate that word $i$ occurred in our review. One thng to not that this loses some valuable sequential information. For instance, the reviews \"bad. not good.\" and \"good. not bad.\" would have the same representation since they contain the same words. However, the first review describes a bad movie and the second review describes a good movie. There are other models that can handle sequential data like this, but this is an okay representation for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "# Vectorize our input data \n",
    "vectorized_train_data = vectorize_sequences(train_data)\n",
    "vectorized_test_data = vectorize_sequences(test_data)\n",
    "\n",
    "# Vectorize our label data as well \n",
    "vectorized_train_labels = np.asarray(train_labels).astype('float32')\n",
    "vectorized_test_labels = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make everything is in order, let's check the shape of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data\n",
      "Train Data (25000, 10000)\n",
      "Test Data (25000, 10000)\n",
      "\n",
      "Output Data\n",
      "Train Labels (25000,)\n",
      "Train Labels (25000,)\n"
     ]
    }
   ],
   "source": [
    "print('Input Data')\n",
    "print('Train Data', vectorized_train_data.shape)\n",
    "print('Test Data', vectorized_test_data.shape)\n",
    "\n",
    "print('\\nOutput Data')\n",
    "print('Train Labels', vectorized_train_labels.shape)\n",
    "print('Train Labels', vectorized_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create our train, validation, and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate input data\n",
    "x_val = vectorized_train_data[:10000].T\n",
    "x_train = vectorized_train_data[10000:].T\n",
    "x_test = vectorized_test_data[:].T\n",
    "\n",
    "# Separate labels \n",
    "y_val = vectorized_train_labels[:10000]\n",
    "y_train = vectorized_train_labels[10000:]\n",
    "y_test = vectorized_test_labels[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Activation and Cost Functions \n",
    "\n",
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    \"\"\"An abstract class representing an activation function.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass \n",
    "    \n",
    "    def forward(self, Z): \n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, dA, Z): \n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "class Sigmoid(ActivationFunction): \n",
    "    \"\"\"Implementation of the sigmoid activation function.\"\"\"\n",
    "    \n",
    "    def forward(self, Z): \n",
    "        return 1.0 / (1.0 + np.exp(-Z))\n",
    "        \n",
    "    def backward(self, dA, Z): \n",
    "        s = 1 / (1+np.exp(-Z))\n",
    "        dZ = dA * s * (1-s)\n",
    "        return dZ\n",
    "    \n",
    "\n",
    "class Tanh(ActivationFunction): \n",
    "    \"\"\"Implementation of the hypterbolic tangent activation function.\"\"\"\n",
    "    \n",
    "    def forward(self, Z): \n",
    "        return np.tanh(Z)\n",
    "        \n",
    "    def backward(self, dA, Z): \n",
    "        t = tanh(Z)\n",
    "        return dA * (1 - t*t)\n",
    "    \n",
    "\n",
    "class ReLU(ActivationFunction): \n",
    "    \"\"\"Implementation of the ReLU function.\"\"\"\n",
    "    \n",
    "    def forward(self, Z): \n",
    "        return np.maximum(Z, 0)\n",
    "        \n",
    "    def backward(self, dA, Z): \n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "        return dZ\n",
    "    \n",
    "\n",
    "class LeakyReLU(ActivationFunction): \n",
    "    \"\"\"Implementation of the Leaky ReLU function.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha): \n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, Z): \n",
    "        return np.maximum(Z, self.alpha*Z)\n",
    "        \n",
    "    def backward(self, dA, Z): \n",
    "        dZ = np.ones_like(Z)\n",
    "        dZ[Z <= 0] = self.alpha\n",
    "        return dA*dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CostFunction:\n",
    "    \"\"\"An abstract class representing a cost function.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass \n",
    "    \n",
    "    def cost(self, AL, Y):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def derivative(self, AL, Y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "class CrossEntropy(CostFunction):\n",
    "    \"\"\"Implementation of the Cross Entropy Cost Function.\"\"\"\n",
    "    \n",
    "    def cost(self, AL, Y):\n",
    "        m = Y.shape[-1]\n",
    "        eps = 1e-8\n",
    "        cost = (-1/m) * np.sum(np.multiply(np.log(AL + eps), Y) + np.multiply(np.log((1-AL) + eps),(1-Y)))\n",
    "        return np.squeeze(cost)\n",
    "    \n",
    "    def derivative(self, AL, Y):\n",
    "        eps = 1e-8\n",
    "        return - (np.divide(Y, AL + eps) - np.divide(1 - Y, 1 - AL + eps))\n",
    "    \n",
    "\n",
    "class MeanSquaredError(CostFunction):\n",
    "    \"\"\"Implementation of the Mean Squared Error Cost Function\"\"\"\n",
    "    \n",
    "    def cost(self, AL, Y):\n",
    "        m = Y.shape[-1]\n",
    "        cost = (1/(2*m)) * np.sum(np.square(Y - AL))\n",
    "        return np.squeeze(cost)\n",
    "    \n",
    "    def derivative(self, AL, Y):\n",
    "        return AL - Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing our Neural Network\n",
    "\n",
    "### Initialization \n",
    "\n",
    "### Forward Propagation \n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "### Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    #\n",
    "    # Initialization\n",
    "    #\n",
    "    \n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"Initializes a new network with the given dimensions.\"\"\"\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.cost_function = CrossEntropy()\n",
    "        self.activation_functions = self.initialize_activation_functions(layer_sizes)\n",
    "        self.parameters = self.initialize_parameters(layer_sizes) \n",
    "        \n",
    "        \n",
    "    def initialize_activation_functions(self, layer_sizes):\n",
    "        \"\"\"Returns an array of activation functions.\"\"\"\n",
    "        activations = [None]\n",
    "        for _ in range(1, self.num_layers-1):\n",
    "            activations.append(ReLU())\n",
    "        activations.append(Sigmoid())\n",
    "        return activations\n",
    "            \n",
    "        \n",
    "    def initialize_parameters(self, layer_sizes):\n",
    "        \"\"\"Returns a new dictionary of weights and biases.\"\"\"\n",
    "        parameters = {}\n",
    "        for l in range(1, self.num_layers):\n",
    "            parameters['W' + str(l)] = np.random.randn(layer_sizes[l], layer_sizes[l-1]) * 0.01\n",
    "            parameters['b' + str(l)] = np.zeros((layer_sizes[l], 1))\n",
    "        return parameters\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # Forward Propagation\n",
    "    #\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"Compute the output for the given examples.\"\"\"\n",
    "        A = X \n",
    "        caches = [None]\n",
    "        \n",
    "        for layer in range(1, self.num_layers):\n",
    "            A_prev = A\n",
    "            W = self.parameters['W' + str(layer)]\n",
    "            b = self.parameters['b' + str(layer)]\n",
    "            activation = self.activation_functions[layer]\n",
    "            A, cache = self.forward_propagation_step(W, A_prev, b, activation)\n",
    "            caches.append(cache)\n",
    "            \n",
    "        return A, caches\n",
    "    \n",
    "    \n",
    "    def forward_propagation_step(self, W, A_prev, b, activation):\n",
    "        \"\"\"Compute the output of a single layer.\"\"\"\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        A = activation.forward(Z)\n",
    "        cache = (W, A_prev, b, Z, A)\n",
    "        return A, cache\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # Backward Propagation \n",
    "    #\n",
    "    \n",
    "    def backward_propagation(self, AL, Y, caches): \n",
    "        \"\"\"Compute the gradients based on the results of forward propagation.\"\"\"\n",
    "        gradients = {}\n",
    "        m = AL.shape[1]\n",
    "        Y = Y.reshape(AL.shape)\n",
    "        eps = 1e-8\n",
    "        \n",
    "        dA = self.cost_function.derivative(AL, Y)\n",
    "        \n",
    "        for layer in reversed(range(1,self.num_layers)):\n",
    "            cache = caches[layer]\n",
    "            activation = self.activation_functions[layer]\n",
    "            dA, dW, db = self.backward_propagation_step(dA, cache, activation)\n",
    "            gradients[\"dA\" + str(layer-1)] = dA \n",
    "            gradients[\"dW\" + str(layer)] = dW \n",
    "            gradients[\"db\" + str(layer)] = db\n",
    "            \n",
    "        return gradients \n",
    "    \n",
    "    \n",
    "    def backward_propagation_step(self, dA, cache, activation):\n",
    "        \"\"\"Compute the gradients of a single layer.\"\"\"\n",
    "        (W, A_prev, b, Z, A) = cache\n",
    "        m = A_prev.shape[1]\n",
    "        dZ = activation.backward(dA, Z)\n",
    "        dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "        db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # Compute Cost \n",
    "    #\n",
    "    \n",
    "    def compute_cost(self, AL, Y):\n",
    "        \"\"\"Computes the overall cost using the network's cost function.\"\"\"\n",
    "        return self.cost_function.cost(AL, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent_update(network, gradients, eta):\n",
    "    parameters = network.parameters\n",
    "    for i in range(1, network.num_layers):\n",
    "        parameters[\"W\" + str(i)] = network.parameters[\"W\" + str(i)] - eta * gradients[\"dW\" + str(i)]\n",
    "        parameters[\"b\" + str(i)] = network.parameters[\"b\" + str(i)] - eta * gradients[\"db\" + str(i)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minibatches(X, Y, batch_size):\n",
    "    \"\"\"Generates shuffled mini batches up to the given batch size.\"\"\"\n",
    "    permutation = np.random.permutation(X.shape[1])\n",
    "    for k in range(0, X.shape[1], batch_size):\n",
    "        batch_columns = permutation[k:k+batch_size]\n",
    "        X_mini = X[:,batch_columns]\n",
    "        Y_mini = Y[batch_columns]\n",
    "        yield (X_mini, Y_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(network, X, Y, epochs, learning_rate, batch_size=128, validation_data=None):\n",
    "    \"\"\"Trains a neural network using standard gradient descent.\"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        cost = 0\n",
    "        for (mini_X, mini_Y) in minibatches(X, Y, batch_size):\n",
    "            AL, caches = network.forward_propagation(mini_X)\n",
    "            gradients = network.backward_propagation(AL, mini_Y, caches)\n",
    "            cost += network.compute_cost(AL, mini_Y)\n",
    "            parameters = gradient_descent_update(network, gradients, learning_rate)\n",
    "        print(\"Training cost on epoch {}: {}\".format(epoch+1, cost))\n",
    "        if validation_data and (epoch+1) % 5 == 0:\n",
    "            (val_X, val_y) = validation_data\n",
    "            for (mini_val_X, mini_val_Y) in minibatches(val_X, val_Y, batch_size):\n",
    "                AL, caches = network.forward_propagation(mini_val_X)\n",
    "                cost += network.compute_cost(AL, mini_Y)\n",
    "            print(\"Validation cost on epoch {}: {}\".format(epoch+1, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cost on epoch 1: 81.81183973260202\n",
      "Training cost on epoch 2: 81.80225538315646\n",
      "Training cost on epoch 3: 81.7751012294881\n",
      "Training cost on epoch 4: 77.46582880480942\n",
      "Training cost on epoch 5: 55.51079593481038\n",
      "Training cost on epoch 6: 40.635461493206876\n",
      "Training cost on epoch 7: 36.007702988856714\n",
      "Training cost on epoch 8: 31.694659472161195\n",
      "Training cost on epoch 9: 29.182836233801\n",
      "Training cost on epoch 10: 26.517782052437877\n",
      "Training cost on epoch 11: 24.978710591139063\n",
      "Training cost on epoch 12: 28.43087660277484\n",
      "Training cost on epoch 13: 22.14125657947487\n",
      "Training cost on epoch 14: 24.96093224206402\n",
      "Training cost on epoch 15: 17.65338243271313\n",
      "Training cost on epoch 16: 30.16677157439399\n",
      "Training cost on epoch 17: 20.70157852340505\n"
     ]
    }
   ],
   "source": [
    "network = Network([10000, 32, 32, 1])\n",
    "train(network, x_train, y_train, 50, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 15000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
