{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korean Character Recognition with Convnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notice\n",
    "This post was delayed due to getting sick, traveling, moving back to college park, etc. It is being updated incrementally and a somewhat complete version will be done by Sunday with suggestions for possible future updates. The main cause of delay was issues with the data size which will be discussed extensivey in the data loading section.\n",
    "\n",
    "Rather than continue to wait for this to be in perfect condition, I am going to update it each day for the next few days.\n",
    "\n",
    "---\n",
    "## Introduction \n",
    "This semester, I am studying abroad at Yonsei University in South Korea. \n",
    "\n",
    "![Yonsei's Campus](./images/yonsei_campus.JPG)\n",
    "\n",
    "I spend two hours per day in Korean class, so I wanted to make at least one post related to Korean. I figured using convnets to recognize Korean characters would be fun, and it's also quite a challenge. There are 10 digits for MNIST and 26 letters for the English alphabet, but the Korean alphabet contains 11,172 possible character combinations. In reality, however, only 2,350 characters are frequently used ([source](https://ko.m.wikipedia.org/wiki/%ED%95%9C%EA%B8%80_%EC%9D%8C%EC%A0%88))\n",
    "\n",
    "> (Translated from Korean) These characters can be expressed in all combinations of Korean characters, but KS X 1001 Korean complete encoding only contains 2,350 characters which are frequently used, so the remaining 8,822 characters cannot be expressed. The recently used extended completion code and Unicode series support all 11,172 characters.\n",
    "\n",
    "In this post, we will use the PHD08 Korean characters dataset which contains 2,187 samples of each of the 2,350 Korean character classes for a total of 5,139,450 data samples. Uncompressed, the datset is 7.52 GB and it's in a... \"unique\" format, so we'll get to spend an entire section reformatting it üëç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## A Brief Introduction to the Korean Alphabet\n",
    "\n",
    "### Introduction \n",
    "Knowledge of the Korean alphabet is not really necessary to train a convnet to recognize Korean characters. After all, once the data is properly formatted, all we have to do is feed it to Keras and let it tell us our loss and accuracy. That being said, the Korean alphabet is pretty cool, and it may be way easier to learn than you think ‚Äì especially compared to other asian languages. For example, Japanese has three alphabets: Kanji which contains 2,136 frequently used Chinese characters, hiragana which contains 42 symbols, and katakana which also contains 42 symbols. For chinese, there are over 50,000 unique characters although a Chinese speaker only needs to know about 8,000.\n",
    "\n",
    "Now you may be thinking, \"Japanese has 2,220, Chinese has 8,000, and Korean has 2,350... Korean doesn't seem any easier.\" But the 2,350 character number is entirely misleading. Korean is a phonetic language, just like English! There are 19 consonants and 21 vowels meaning there are only 40 letters (a few more than English which has 26). The reason there are 2,350 characters is because the letters are combined into syllable blocks. Since syllable blocks can contain 2, 3, or 4 letters, there are 2,350 possible syllable blocks/characters. Let's look at an example. \n",
    "\n",
    "### Example\n",
    "<img src=\"./images/hello.png\" alt=\"ÏïàÎÖïÌïòÏÑ∏Ïöî Spelling\" style=\"width: 300px;\"/>\n",
    "\n",
    "The image above is the word ÏïàÎÖïÌïòÏÑ∏Ïöî (an-nyeong-ha-se-yo) which is the most common greeting in Korean ‚Äì equivalent to \"Hello\" in English. The word is 5 syllables long, so it contains 5 different syllable blocks (characters): (1) Ïïà an, (2) ÎÖï nyeong, (3) Ìïò ha, (4) ÏÑ∏ se, (5) Ïöî yo. Let's break each of these syllable blocks into their individual letters.\n",
    "\n",
    "+ Ïïà \"an\"\n",
    "    + „Öá Consonant. At the beginning of a syllable it makes no sound (all syllables must start with consonants)\n",
    "    + „Öè Vowel.  'aw' sound\n",
    "    + „Ñ¥ Consonant. 'n' sound, points **n**orth and east. \n",
    "+ ÎÖï \"nyeong\"\n",
    "    + „Ñ¥ Consonant. 'n' sound, points **n**orth and east. \n",
    "    + „Öï Vowel. 'yeo' sound\n",
    "    + „Öá Consonant. At the end of a word this makes an 'ng' sound like in 'ing' in English\n",
    "+ Ìïò \"ha\" \n",
    "    + „Öé Consonant. 'h' sound, looks like a man in a **h**at\n",
    "    + „Öè Vowel. 'aw' sound \n",
    "+ ÏÑ∏ \"se\"\n",
    "    + „ÖÖ Consonant, 's' sound, looks like a person doing a **s**plit\n",
    "    + „Öî Vowel. 'eh' sound like the end of the word 'say' \n",
    "+ Ïöî \"yo\"\n",
    "    + „Öá Consonant. Silent at the beginning of a syllable. \n",
    "    + „Öõ Vowel. 'yo' sound. \n",
    "    \n",
    "### Korean History\n",
    "The Korean alphabet, which is called Hangeul (ÌïúÍ∏Ä), was created in the 15th century by the 4th king of the Joseon dynasty ‚Äì King Sejong the Great. Prior to the invention of Hanguel, Korean was written using Chinese characters. Due to the large number of characters required, there was low literacy among common people. King Sejong and his linguists created the script to promote literacy among commoners and establish a cultural identity for Korea through its script. The script was finished in 1443 and published in 1446. A book, called the Hunminjeongeum (ÌõàÎØºÏ†ïÏùå), was also published to teach the new scripts. The Hunminjeongeum was published on October 9, 1446. October 9 is a commemorative holiday known as Hangeul Day (ÌïúÍ∏ÄÎÇ†) in South Korea. By the way, here is a fun, little known fact. Hangeul Day is celebrated on January 15th in North Korea and referred to as Choseongeul Day. January 15th is the day that Hangeul was invented. October 9th is the day that Hanguel was proclaimed via the Hunminjeongeum. \n",
    "\n",
    "In 2009, the following sculpture of King Sejong was raised in Gwanghwamun Plaza in Seoul.\n",
    "![Statue of King Sejong](./images/KingSejong.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preparing the Data\n",
    "The biggest challenge with this project was getting the data into a usable form. As you will see below, the data was in an unusual file format and zipped with a non-standard archive tool. Simply unzipping the data was a challenge. Once I got the data unzipped, I had to parse it. From there I had a number of decisions. Each one is explained below. \n",
    "\n",
    "\n",
    "### Option 1: Reformat the data as images, then upload to Google \n",
    "My first idea was to parse the data, organize it into training/validation/testing directories, and save 2d numpy arrays as images. This was my preferred solution as it makes using data augmentation simple and the images are in an easy to understand format. The problem was that the final dataset contained 5,139,450 images (2,350 classes * 2,187 samples per class) and was 24GB. The code took hours to run, and when it was finished I realized the fatal flaw to this approach... Google Drive file read/writes are S L O W. I expected uploading 24GB in the form of 5,139,450 images to take a while, but I didn't expect the estimated time-to-upload to be 41 days (actually it said 999 hours which appears to be the max estimate). \n",
    "\n",
    "However, I am quite proud of the code for this solution. It did work well, and this data format is significantly more convenient. If you have your own deep learning machine, then I would suggest this approach. Alternatively, if you don't mind paying for storage, I would recommend uploading the files to AWS or Paperspace and doing the image parsing there. \n",
    "\n",
    "#### Downloading the Data\n",
    "You can download the data from its [original provider](http://cv.jbnu.ac.kr/index.php?mid=notice&document_srl=189) or use this direct [Dropbox link](https://www.dropbox.com/s/69cwkkqt4m1xl55/phd08.alz?dl=0). \n",
    "\n",
    "#### Unzipping the Data\n",
    "The data is in some propriety `.alz` form. If you're on Windows, you can unzip this using ALZip. If you are on Mac, I recommend using Unarchiver. If you are asked for the encoding format when unzipping in, select \"(MS, DOC) Korean\" which should show an output file like Í∞Ä.txt.\n",
    "\n",
    "![You may not like it, but this is what peak proprietary formats look like](./images/unzip.jpg)\n",
    "\n",
    "#### Inspecting the Data\n",
    "Go ahead and look at the output files. You'll notice that there are 2,350 `.txt` files ‚Äì one for each Korean character. Inside the text files are 2,187 samples of the character represented in the following format: `sample id`, `dimensions` (rows then columns), `binary representation`. If you squint hard enough at the example below, you should see that it resembles the character Í∞Ä. \n",
    "\n",
    "```\n",
    "s_0_0_0_0_1\n",
    "22 29\n",
    "00000000000000000000011100000\n",
    "00000000000000000000011100000\n",
    "11111111111111100000011100000\n",
    "11111111111111100000011100000\n",
    "00000000000011100000011100000\n",
    "00000000000001100000011000000\n",
    "00000000000011100000011100000\n",
    "00000000000011100000011000000\n",
    "00000000000001100000011100000\n",
    "00000000000001100000011111111\n",
    "00000000000011100000011111111\n",
    "00000000000011100000011100000\n",
    "00000000000011100000011000000\n",
    "00000000000000000000011100000\n",
    "00000000000000000000011100000\n",
    "00000000000000000000011000000\n",
    "00000000000000000000011000000\n",
    "00000000000000000000011000000\n",
    "00000000000000000000011000000\n",
    "00000000000000000000011000000\n",
    "00000000000000000000011100000\n",
    "00000000000000000000011000000\n",
    "```\n",
    "\n",
    "#### What We Need To Do\n",
    "We need to complete the following steps\n",
    "1. Write a file parser to turn each sample into a 2D numpy array\n",
    "2. Write a function to save all of the 2D numpy arrays as images \n",
    "3. Iterate through all the files and save the images to train/validation/test folders\n",
    "\n",
    "#### Parsing Files Into Numpy Arrays\n",
    "The following code will parse the file and turn each image into a Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np \n",
    "from scipy.misc import imresize\n",
    "\n",
    "def load_images(file):\n",
    "    \"\"\"Return all the characters as a list of 2D numpy arrays\"\"\"\n",
    "    # Precompile match patterns for reuse \n",
    "    image_dimensions_regex = re.compile(r'^(\\d+) (\\d+)$')\n",
    "    image_binary_regex = re.compile(r'^(\\d+)$')\n",
    "\n",
    "    # Return all images from file \n",
    "    images = [] \n",
    "\n",
    "    # Open the file \n",
    "    with open(file, 'r') as file:\n",
    "        sample_id = 0 \n",
    "        image = None \n",
    "        for line in file: \n",
    "            # Blank Lines: Add image to list \n",
    "            if line == '\\n':\n",
    "                if image is not None:\n",
    "                    images.append(image)\n",
    "                    image = None\n",
    "                continue \n",
    "\n",
    "            # Sample IDs: Increment the sample number \n",
    "            if '_' in line:\n",
    "                sample_id += 1\n",
    "                continue \n",
    "\n",
    "            # Image Dimensions: Create numpy array \n",
    "            dims = re.match(image_dimensions_regex, line)\n",
    "            if dims:\n",
    "                rows = int(dims.group(1))\n",
    "                cols = int(dims.group(2))\n",
    "                row_index = iter(range(rows))\n",
    "                image = np.zeros((rows, cols))\n",
    "\n",
    "            # Binary Image: Add each row to array\n",
    "            row_data = re.match(image_binary_regex, line)\n",
    "            if row_data:\n",
    "                row = next(row_index)\n",
    "                data = [int(c) for c in list(row_data.group(1))]\n",
    "                image[row] = data\n",
    "\n",
    "    return images          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a list of images and an output directory, this code will save each image as a jpeg. Since our images were 0 or 1 before, we multiply them by 255. This means we will have to scale them when we load them into our convnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def save_images(images, output_dir):\n",
    "    \"\"\"Saves an array of numpy images to the specified output directory\"\"\"\n",
    "    for (idx, image) in enumerate(images):\n",
    "        img = Image.fromarray(image*255.).convert(\"L\")\n",
    "        img_name = str(idx) + '.jpg'\n",
    "        output_path = os.path.join(output_dir, img_name)\n",
    "        img.save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this script creates all the output directories, iterates through all the files, and saves the images accordingly. You should change the source directory and the target directory based on your information. Also, we use 1,187 images for training, 500 images for validation, and the remaining (500) images for testing. You can adjust this if you want as well. \n",
    "\n",
    "NOTE: This took 2 hours and 10 minutes to run on my MacBook Pro üëéü§ïüëé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# UPDATE THESE  \n",
    "# ------------------------------------------------------------\n",
    "phd08_source_dir = '/Users/jtbergman/Datasets/phd08/'\n",
    "phd08_target_dir = '/users/jtbergman/Datasets/phd08formatted'\n",
    "\n",
    "# Paths for train, test, validation directories \n",
    "train_dir = os.path.join(phd08_target_dir, 'train')\n",
    "test_dir = os.path.join(phd08_target_dir, 'test')\n",
    "val_dir = os.path.join(phd08_target_dir, 'validation')\n",
    "\n",
    "# Train / Val / Test split\n",
    "train_split = 1187 \n",
    "val_split = 500\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# DON'T CHANGE   \n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def mkdir(directories):\n",
    "    \"\"\"Create directories if they don't already exist.\"\"\"\n",
    "    if type(directories) != list:\n",
    "        directories = [directories]\n",
    "    for d in directories:\n",
    "        if not os.path.exists(d):\n",
    "            os.mkdir(d)\n",
    "\n",
    "mkdir([phd08_target_dir, train_dir, test_dir, val_dir])\n",
    "\n",
    "def output_directories_for_file(file):\n",
    "    \"\"\"Return output directories for a character's images.\"\"\"\n",
    "    filename = os.path.basename(file)\n",
    "    character = filename.split('.')[0]\n",
    "    train_out = os.path.join(train_dir, character) \n",
    "    test_out = os.path.join(test_dir, character)\n",
    "    val_out = os.path.join(val_dir, character)\n",
    "    mkdir([train_out, test_out, val_out])\n",
    "    return train_out, test_out, val_out \n",
    "\n",
    "# Iterate over all the files and save them to train/dev/test\n",
    "for file in os.listdir(phd08_source_dir):\n",
    "    train, test, val = output_directories_for_file(file)\n",
    "    images = load_images(os.path.join(phd08_source_dir,file))\n",
    "    save_images(images[:train_split], train)\n",
    "    save_images(images[train_split:train_split+val_split], val)\n",
    "    save_images(images[train_split+val_split:], test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Upload the files to Google Drive, then convert to Images\n",
    "After realizing it was going to be impossible to upload the dataset to Google Drive, I decided to upload the text files to Google Drive and convert them into images on Google Drive. I thought the input/output would be faster with Colaboratory. Uploading all 2,350 text files was estimated to take something like 16 hours, but really only took 3. Once they were uploaded, I ran the same code as above and simply changed the input and output directories to the ones on Google Drive. \n",
    "\n",
    "As mentioned in the previous section, processing the data only took 2 hours and 10 minutes on my MacBook. But my MacBook has a solid-state drive and 16GB of RAM. I'm not sure what Google Drive has, but I'm sure it's not as fast. The file reading and writing is the bottleneck here. After running the code for several hours it appeared that many of the images were not even showing up. I had to wait almost 24 hours for images to stop showing up so I could completely delete them from my account. \n",
    "\n",
    "I recommend you don't even try this. Although, as mentioned in the previous section, this may be a viable solution on Paperspace and AWS where I imagine the read/writes are much faster. Just beware the output is 24 GB (+7GB for the files), so you'll need to be prepared for storage and the GPU time to convert the images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Skip the image conversion, just load the data as numpy arrays\n",
    "At this point, I already had the files uploaded to Google Drive. I knew this was my only hope of using the data in Colaboratory, so I had to find a way to make it work. I decided instead of parsing the files and converting them to images, I would just load the files directly into training, validation, and testing numpy arrays. \n",
    "\n",
    "The problem with this is the images need to be 150x150 to be used by most CNN architetures easily available in Keras. Furthermore, the images are supposed to be in RGB format, so we have to duplicate the same image 3 times. If we were to load all of our data into a single numpy array it would have the shape `(5139450, 150, 150, 3)`... that's big. In fact, you can only parse 45 of the 2,350 files before you use the 15GB available to you on Google Colaboratory. If you just loaded the training data you could load 90 files ‚Äì almost 4% of our data üò≠ \n",
    "\n",
    "To make matters worse, each file was taking about 10 seconds with the reformatting which comes out to about 6.5 hours. When you run a colab file for too long, Google will kill it and wipe all your variables. Thus, even if you could load the data Google would almost certainly cut you off and erase your progress. \n",
    "\n",
    "### Option 4: Load, train, repeat\n",
    "I realized if I wanted to use Colaboratory I was going to have to find a creative solution. \n",
    "\n",
    "My first improvement was to reduce the image size. Currently the images are `(150, 150, 3)` which corresponds to 67,500 integers. I decided to use a pre-trained model (which will be shown below) and feed each image through the convolutional base when it was first loaded. Then, instead of storing a `(150, 150, 3)` image I could store a `(1, 3, 3, 2048)` or a `(1,2048)` tensor depending on whether or not I used max-pooling on the final layer. These correspond to 18,432 or 2,048 integer values respectively ‚Äì a significant reduction from 67,500. \n",
    "\n",
    "My second idea was to only load a few samples from each class per training iteration. For instance, I would load 25 randomly selected images from each class, feed each through the convolutional base when loaded, then train a simple model for 20 epochs on this dataset. After that, I would save the model, load 25 more images from each class, then train the model again. This was the solution I ended up using. As you'll see, it's not perfect... but it \"works\".\n",
    "\n",
    "First, I defined a dictionary to map each filename to a class label `(0...2349)`. I also reversed the dictionary to map each label to its filename. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Map each filename to a label\n",
    "filename_to_label = {} \n",
    "for (idx,filename) in enumerate(os.listdir(phd08_source_dir)):\n",
    "  filename_to_label[filename] = idx\n",
    "\n",
    "# Map each label to a filename \n",
    "label_to_filename = {v: k for k,v in filename_to_label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I implemented a modified version of the the `load_images` function which took a filename and a `Set` of sample ids to keep. The time and space improvements here come from limiting the number of samples being loaded, only allocating a numpy array for the specified samples, and stopping early after the last sample is loaded. Most important for our model is the last line before the return, `features = conv_base.predict(np.array(images)/255.0)`. This is where we get the reduction from 67,500 integers to either 18,432 or 2,048 depending on the conv_base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np \n",
    "from scipy.misc import imresize\n",
    "\n",
    "img_size = 150\n",
    "\n",
    "def load_features(conv_base, file, samples):\n",
    "    \"\"\"Return all the characters as a list of 2D numpy arrays\"\"\"\n",
    "    # Precompile match patterns for reuse \n",
    "    image_dimensions_regex = re.compile(r'^(\\d+) (\\d+)$')\n",
    "    image_binary_regex = re.compile(r'^(\\d+)$')\n",
    "\n",
    "    # Return all images from file \n",
    "    images = [] \n",
    "\n",
    "    # Open the file \n",
    "    with open(file, 'r') as file:\n",
    "        sample_id = 0 \n",
    "        max_sample_id = max(samples)\n",
    "        image = None \n",
    "        for line in file: \n",
    "            # Blank Lines: Add image to list \n",
    "            if line == '\\n':\n",
    "                if image is not None:\n",
    "                    image_resize = imresize(image, size=(img_size, img_size), interp='bilinear')\n",
    "                    image_resize = np.stack((image_resize,)*3, axis=-1)\n",
    "                    images.append(image_resize)\n",
    "                    image = None\n",
    "                continue \n",
    "\n",
    "            # Sample IDs: Increment the sample number \n",
    "            if '_' in line:\n",
    "                sample_id += 1\n",
    "                continue \n",
    "                \n",
    "            if sample_id > max_sample_id:\n",
    "                break\n",
    "            \n",
    "            if sample_id in samples: \n",
    "                # Image Dimensions: Create numpy array \n",
    "                dims = re.match(image_dimensions_regex, line)\n",
    "                if dims:\n",
    "                    rows = int(dims.group(1))\n",
    "                    cols = int(dims.group(2))\n",
    "                    row_index = iter(range(rows))\n",
    "                    image = np.zeros((rows, cols))\n",
    "\n",
    "                # Binary Image: Add each row to array\n",
    "                row_data = re.match(image_binary_regex, line)\n",
    "                if row_data:\n",
    "                    row = next(row_index)\n",
    "                    data = [int(c) for c in list(row_data.group(1))]\n",
    "                    image[row] = data\n",
    "    \n",
    "    features = conv_base.predict(np.array(images)/255.0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we define the massive function to load a few images per iteration and train our model, we are going to write a small generator to give us sample ids to load. Each time this generator it will return a set of sample ids that have not been loaded previously. We will load those samples from every class. There's no reason, but also no harm, in using the same samples from each class on an iteration. I decided to use random sample ids rather than loading images in order in case similar sample ids contain similar characters ‚Äì I'm not sure if that's the case or not, but it did appear that way to me. \n",
    "\n",
    "One drawback is that the sample_range is usually set to 550 so that we can stop after the 550th sample id... this means if we test our model using data outside this range it may be a slightly different distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_sample_ids(sample_range, num_samples, num_iterations):\n",
    "  \"\"\"Generate non-repeating sample ids in a given range.\n",
    "  \n",
    "  Arguments:\n",
    "    - sample_range    The possible range of values\n",
    "    - num_samples     The number of sample ids to return per call \n",
    "    - num_iterations  How many times the generator can be called\n",
    " \"\"\"\n",
    "  i = 0\n",
    "  samples = np.random.permutation(sample_range)\n",
    "  while i < num_iterations:\n",
    "    yield samples[i*num_samples : (i+1)*num_samples]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here is the function that loads the samples, trains our model, and repeats the extremely long process. \n",
    "\n",
    "This line `print('\\rLoaded classes:', loaded, end='', flush=True)` is a nifty trick to print the number of classes loaded using only one output line (it updates the same line during each print call). This allows us to maintain our sanity that our code is still running (albeit slowly) without spamming our output with prints. \n",
    "\n",
    "If you want to further preserve your sanity, you may want to change `verbose=0` in `hist = model.fit(features, labels, epochs=epochs, verbose=0)` to either `verbose=1` or `verbose=2` otherwise you won't see anything while your model is training. I guess `verbose=2` would probably be better as it just prints one update per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, conv_base, feature_dims, samples_per_class=25, number_of_iterations=20, epochs=20):\n",
    "    \"\"\"Trains our model using a small number of samples from each class.\n",
    "\n",
    "    This helper function will train our model. Suppose we use the default \n",
    "    number of iterations, 20, and the default number of samples per class, 30. \n",
    "    Then our model will select 30 images from each of the 2,350 classes and \n",
    "    train the model on them for 30 epochs. It will then repeat this process for a\n",
    "    total of 20 iterations. After each iteration it will display the accuracy \n",
    "    and the predictions of the model on a set of examples.\n",
    "\n",
    "    Arguments\n",
    "    - model                 The model we're training \n",
    "    - conv_base             The conv_base to use to extract features from the samples\n",
    "    - feature_dims          The dimension of the inputs to the model/outputs of conv base when reshaped\n",
    "    - samples_per_class     The number of samples from each of 2,350 classes\n",
    "    - number_of_iterations  The number of times we sample and retrain \n",
    "    - epochs                Epochs per iteration \n",
    "    \"\"\"\n",
    "\n",
    "    phd08_source_dir = '/content/drive/My Drive/Colab Notebooks/Datasets/phd08/'\n",
    "    num_classes = 2350\n",
    "    training_sample_range = 550 \n",
    "    sample_count = num_classes * samples_per_class\n",
    "    accuracy_history = [] \n",
    "    first_ten_prediction_history = []\n",
    "    unique_ten_prediction_history = []\n",
    "\n",
    "    i = 1\n",
    "    for samples in get_sample_ids(training_sample_range, samples_per_class, number_of_iterations):\n",
    "        print('STARTING ITERATION', i)\n",
    "        print('===================' + len(str(i)) * '=')\n",
    "        print('')\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # Preallocate space for the labels and the extracted features\n",
    "        # -----------------------------------------------------------\n",
    "        features = np.zeros((sample_count, feature_dims))\n",
    "        labels = np.zeros((sample_count))\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Perform feature extraction for the specified samples \n",
    "        # ----------------------------------------------------\n",
    "        loaded = 0\n",
    "        for file in os.listdir(phd08_source_dir):\n",
    "            extracted_features = load_features(conv_base, os.path.join(phd08_source_dir,file), samples)\n",
    "            extracted_features = np.reshape(extracted_features, (extracted_features.shape[0], feature_dims))\n",
    "            features[loaded * samples_per_class : (loaded+1) * samples_per_class] = extracted_features\n",
    "            labels[loaded * samples_per_class : (loaded+1) * samples_per_class] = np.array([filename_to_label[file]] * len(extracted_features))\n",
    "            loaded += 1\n",
    "            print('\\rLoaded classes:', loaded, end='', flush=True)\n",
    "        labels = to_categorical(labels)\n",
    "        print('\\n')\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Fit to the loaded samples \n",
    "        # ----------------------------------------------------\n",
    "        hist = model.fit(features, labels, epochs=epochs, verbose=0)\n",
    "        accuracy = hist.history['acc']\n",
    "        accuracy_history += accuracy\n",
    "        print('Accuracy:', accuracy[-1], end='\\n\\n')\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Check predictions on first 10 samples\n",
    "        # ----------------------------------------------------\n",
    "        print('Predictions on first 10 samples')\n",
    "        print('-------------------------------')\n",
    "        for sample_number in range(10):\n",
    "            actual_class_id = np.argmax(labels[sample_number])\n",
    "            actual_class_name = label_to_filename[actual_class_id].split('.')[0]\n",
    "            print('Actual Class:', actual_class_name)\n",
    "            predicted_class_id = model.predict_classes(np.expand_dims(features[sample_number], axis=0))[0]\n",
    "            predicted_class_name = label_to_filename[predicted_class_id].split('.')[0]\n",
    "            print('Predicted Class:', predicted_class_name)\n",
    "            first_ten_prediction_history.append((actual_class_name, predicted_class_name))\n",
    "            print('')\n",
    "        print('')\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Check predictions on 10 unique samples\n",
    "        # ----------------------------------------------------\n",
    "        print(\"Predictions on 10 unique samples\")\n",
    "        print('--------------------------------')\n",
    "        for sample_number in range(0,100,samples_per_class):\n",
    "            actual_class_id = np.argmax(labels[sample_number])\n",
    "            actual_class_name = label_to_filename[actual_class_id].split('.')[0]\n",
    "            print('Actual Class:', actual_class_name)\n",
    "            predicted_class_id = model.predict_classes(np.expand_dims(features[sample_number], axis=0))[0]\n",
    "            predicted_class_name = label_to_filename[predicted_class_id].split('.')[0]\n",
    "            print('Predicted Class:', predicted_class_name)\n",
    "            unique_ten_prediction_history.append((actual_class_name, predicted_class_name))\n",
    "            print('')\n",
    "\n",
    "        print('')\n",
    "        model.save('/content/drive/My Drive/Colab Notebooks/phd08_model.h5')\n",
    "        print('Model Saved')\n",
    "\n",
    "        print('\\n\\n')\n",
    "        i += 1\n",
    "\n",
    "    return accuracy_history, first_ten_prediction_history, unique_ten_prediction_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My initial thoughts with this function was, \"it takes so long to load data, I might as well train a lot on it.\" I found that with 20 epochs I could get really accuracy on those samples, but there was pretty extreme overfitting. The training takes a non-trivial amount of time itself, so it may be better to just do 1-3 epochs and get on to loading your next dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Architecture Decisions\n",
    "\n",
    "### Introduction\n",
    "This post, and the usage of the PHD08 dataset, was inspired by [_Variations of AlexNet and GoogLeNet to Improve Korean Character Recognition Performance_](http://jips-k.org/q.jips?cp=pp&pn=537). They trained to networks KCR-Alexnet and KCR-GoogLeNet for Korean Character Recognition (hence the KCR prefix). \n",
    "\n",
    "The KCR-AlexNet architecture is described as follows.\n",
    "> The overall architecture of KCR-AlexNet is the same as AlexNet, but KCR-AlexNet uses a 56√ó56-pixel input data size for Korean character images, which is smaller than AlexNet‚Äôs input data size of 256√ó256 for natural images. In addition, while the output layer of the existing AlexNet only has 1,000 nodes for classifying ILSVRC‚Äôs classes, KCR-AlexNet needs 2,350 nodes at the output layer to classify PHD08‚Äôs 2,350 Korean character classes.\n",
    "\n",
    "The KCR-GoogLeNet architecture is described as follows. \n",
    "> The biggest difference between GoogLeNet and KCR-GoogLeNet is that GoogLeNet uses nine inception modules and KCR-GoogLeNet uses only three inception modules. This is because GoogLeNet‚Äôs purpose is to classify the nature of a 256√ó256√ó3 image size and KCR-GoogLeNet‚Äôs purpose is to classify small Korean characters of size 56√ó56√ó1. The KCR-GoogLeNet architecture is shown in Fig. 3 and the detail size of each layer, including the inception modules, is introduced in Table 1.\n",
    "\n",
    "### Our Model \n",
    "We are going to add to the mix by training KCR-ResNet50. That is, we'll be using the ResNet50 architecture (provided by Keras) for Korean Character Recognition. To match the paper, we will also resize our inputs to 56x56x1 and we will have to replace the top layers with a 2,350 dimension softmax classifier. Although the ImageNet images aren't very similar to Korean characters, we are still going to use the ImageNet weights. \n",
    "\n",
    "### Training Approach \n",
    "1. We will train a small FCNN using the outputs of ResNet. \n",
    "2. We will add the FCNN to the ResNet model. \n",
    "3. We will \"fine tune\" some of the convolutional base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jtbergman/miniconda2/envs/ml/lib/python3.6/site-packages/keras_applications/resnet50.py:263: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 200, 200, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 206, 206, 3)  0           input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 100, 100, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 100, 100, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_687 (Activation)     (None, 100, 100, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 49, 49, 64)   0           activation_687[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 49, 49, 64)   4160        max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 49, 49, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_688 (Activation)     (None, 49, 49, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 49, 49, 64)   36928       activation_688[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 49, 49, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_689 (Activation)     (None, 49, 49, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 49, 49, 256)  16640       activation_689[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 49, 49, 256)  16640       max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 49, 49, 256)  1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 49, 49, 256)  1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_225 (Add)                   (None, 49, 49, 256)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_690 (Activation)     (None, 49, 49, 256)  0           add_225[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 49, 49, 64)   16448       activation_690[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 49, 49, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_691 (Activation)     (None, 49, 49, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 49, 49, 64)   36928       activation_691[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 49, 49, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_692 (Activation)     (None, 49, 49, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 49, 49, 256)  16640       activation_692[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 49, 49, 256)  1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_226 (Add)                   (None, 49, 49, 256)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_690[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_693 (Activation)     (None, 49, 49, 256)  0           add_226[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 49, 49, 64)   16448       activation_693[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 49, 49, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_694 (Activation)     (None, 49, 49, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 49, 49, 64)   36928       activation_694[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 49, 49, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_695 (Activation)     (None, 49, 49, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 49, 49, 256)  16640       activation_695[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 49, 49, 256)  1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_227 (Add)                   (None, 49, 49, 256)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_693[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_696 (Activation)     (None, 49, 49, 256)  0           add_227[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 25, 25, 128)  32896       activation_696[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 25, 25, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_697 (Activation)     (None, 25, 25, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 25, 25, 128)  147584      activation_697[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 25, 25, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_698 (Activation)     (None, 25, 25, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 25, 25, 512)  66048       activation_698[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 25, 25, 512)  131584      activation_696[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 25, 25, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 25, 25, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_228 (Add)                   (None, 25, 25, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_699 (Activation)     (None, 25, 25, 512)  0           add_228[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 25, 25, 128)  65664       activation_699[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 25, 25, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_700 (Activation)     (None, 25, 25, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 25, 25, 128)  147584      activation_700[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 25, 25, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_701 (Activation)     (None, 25, 25, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 25, 25, 512)  66048       activation_701[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 25, 25, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_229 (Add)                   (None, 25, 25, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_699[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_702 (Activation)     (None, 25, 25, 512)  0           add_229[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 25, 25, 128)  65664       activation_702[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 25, 25, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_703 (Activation)     (None, 25, 25, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 25, 25, 128)  147584      activation_703[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 25, 25, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_704 (Activation)     (None, 25, 25, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 25, 25, 512)  66048       activation_704[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 25, 25, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_230 (Add)                   (None, 25, 25, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_702[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_705 (Activation)     (None, 25, 25, 512)  0           add_230[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 25, 25, 128)  65664       activation_705[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 25, 25, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_706 (Activation)     (None, 25, 25, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 25, 25, 128)  147584      activation_706[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 25, 25, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_707 (Activation)     (None, 25, 25, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 25, 25, 512)  66048       activation_707[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 25, 25, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_231 (Add)                   (None, 25, 25, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_705[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_708 (Activation)     (None, 25, 25, 512)  0           add_231[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 13, 13, 256)  131328      activation_708[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_709 (Activation)     (None, 13, 13, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_709[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_710 (Activation)     (None, 13, 13, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_710[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 13, 13, 1024) 525312      activation_708[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 13, 13, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_232 (Add)                   (None, 13, 13, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_711 (Activation)     (None, 13, 13, 1024) 0           add_232[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 13, 13, 256)  262400      activation_711[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_712 (Activation)     (None, 13, 13, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_712[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_713 (Activation)     (None, 13, 13, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_713[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_233 (Add)                   (None, 13, 13, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_711[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_714 (Activation)     (None, 13, 13, 1024) 0           add_233[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 13, 13, 256)  262400      activation_714[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_715 (Activation)     (None, 13, 13, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_715[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_716 (Activation)     (None, 13, 13, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_716[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_234 (Add)                   (None, 13, 13, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_714[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_717 (Activation)     (None, 13, 13, 1024) 0           add_234[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 13, 13, 256)  262400      activation_717[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_718 (Activation)     (None, 13, 13, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_718[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_719 (Activation)     (None, 13, 13, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_719[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_235 (Add)                   (None, 13, 13, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_717[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_720 (Activation)     (None, 13, 13, 1024) 0           add_235[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 13, 13, 256)  262400      activation_720[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_721 (Activation)     (None, 13, 13, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_721[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_722 (Activation)     (None, 13, 13, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_722[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_236 (Add)                   (None, 13, 13, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_720[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_723 (Activation)     (None, 13, 13, 1024) 0           add_236[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 13, 13, 256)  262400      activation_723[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 13, 13, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_724 (Activation)     (None, 13, 13, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 13, 13, 256)  590080      activation_724[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 13, 13, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_725 (Activation)     (None, 13, 13, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 13, 13, 1024) 263168      activation_725[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 13, 13, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_237 (Add)                   (None, 13, 13, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_723[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_726 (Activation)     (None, 13, 13, 1024) 0           add_237[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 7, 7, 512)    524800      activation_726[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_727 (Activation)     (None, 7, 7, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_727[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_728 (Activation)     (None, 7, 7, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_728[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 7, 7, 2048)   2099200     activation_726[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 7, 7, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_238 (Add)                   (None, 7, 7, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_729 (Activation)     (None, 7, 7, 2048)   0           add_238[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_729[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_730 (Activation)     (None, 7, 7, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_730[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_731 (Activation)     (None, 7, 7, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_731[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_239 (Add)                   (None, 7, 7, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_729[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_732 (Activation)     (None, 7, 7, 2048)   0           add_239[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_732[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_733 (Activation)     (None, 7, 7, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_733[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_734 (Activation)     (None, 7, 7, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_734[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_240 (Add)                   (None, 7, 7, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_732[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_735 (Activation)     (None, 7, 7, 2048)   0           add_240[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 1, 1, 2048)   0           activation_735[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.models import Model\n",
    "\n",
    "resnet = ResNet50(include_top=False, weights='imagenet', input_shape=(200, 200, 3))\n",
    "output = layers.AveragePooling2D(pool_size=(7, 7), padding='valid')(resnet.output)\n",
    "resnet_base = Model(resnet.input, output)\n",
    "resnet_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training KCR-ResNet50\n",
    "\n",
    "### Training a Dense Network\n",
    "First, we will train a dense network using the outputs of ResNet50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models \n",
    "from keras import layers \n",
    "from keras import optimizers\n",
    "\n",
    "# Define the model \n",
    "resnet_top = models.Sequential() \n",
    "resnet_top.add(layers.Dense(2350, activation='softmax', input_dim=1*1*2048))\n",
    "\n",
    "# Compile the Model \n",
    "resnet_top.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "# Train the Model \n",
    "# history = resnet_top.fit(train_images, train_labels,\n",
    "#                          epochs=30,\n",
    "#                          batch_size=32,\n",
    "#                          validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining ResNet with our Dense Network\n",
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kcr_resnet = models.Sequential() \n",
    "kcr_resnet.add(resnet_base)\n",
    "kcr_resnet.add(layers.Flatten())\n",
    "kcr_resnet.add(resnet_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning our Model \n",
    "Freezing some layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_base.trainable = True\n",
    "\n",
    "set_trainable = 'res5c_branch2a'\n",
    "for layer in resnet_base.layers:\n",
    "    if layer.name == set_trainable:\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
