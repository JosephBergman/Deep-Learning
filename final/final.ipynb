{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korean Character Recognition with Convnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction \n",
    "This semester, I am studying abroad at Yonsei University in South Korea. \n",
    "\n",
    "![Yonsei's Campus](./images/yonsei_campus.JPG)\n",
    "\n",
    "I spend two hours per day in Korean class, so I wanted to make at least one post related to Korean. I figured using convnets to recognize Korean characters would be fun, and it's also quite a challenge. There are 10 digits for MNIST and 26 letters for the English alphabet, but the Korean alphabet contains 11,172 possible character combinations. In reality, however, only 2,350 characters are frequently used ([source](https://ko.m.wikipedia.org/wiki/%ED%95%9C%EA%B8%80_%EC%9D%8C%EC%A0%88))\n",
    "\n",
    "> (Translated from Korean) These characters can be expressed in all combinations of Korean characters, but KS X 1001 Korean complete encoding only contains 2,350 characters which are frequently used, so the remaining 8,822 characters cannot be expressed. The recently used extended completion code and Unicode series support all 11,172 characters.\n",
    "\n",
    "In this post, we will use the PHD08 Korean characters dataset which contains 2,187 samples of each of the 2,350 Korean character classes for a total of 5,139,450 data samples. Uncompressed, the datset is 7.52 GB and it's in a... \"unique\" format, so we'll get to spend an entire section reformatting it üëç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preparing the Data\n",
    "\n",
    "### Downloading the Data\n",
    "You can download the data from its [original provider](http://cv.jbnu.ac.kr/index.php?mid=notice&document_srl=189) or use this direct [Dropbox link](https://www.dropbox.com/s/69cwkkqt4m1xl55/phd08.alz?dl=0). \n",
    "\n",
    "### Unzipping the Data\n",
    "The data is in some propriety `.alz` form. If you're on Windows, you can unzip this using ALZip. If you are on Mac, I recommend using Unarchiver. If you are asked for the encoding format when unzipping in, select \"(MS, DOC) Korean\" which should show an output file like Í∞Ä.txt.\n",
    "\n",
    "![You may not like it, but this is what peak proprietary formats look like](./images/unzip.jpg)\n",
    "\n",
    "### Inspecting the Data\n",
    "Go ahead and look at the output files. You'll notice that there are 2,350 `.txt` files ‚Äì one for each Korean character. Inside the text files are 2,187 samples of the character represented in the following format: `sample id`, `dimensions` (rows then columns), `binary representation`. If you squint hard enough at the example below, you should see that it resembles the character Í∞Ä. \n",
    "\n",
    "```\n",
    "s_0_0_0_0_1\n",
    "22 29\n",
    "00000000000000000000011100000\n",
    "00000000000000000000011100000\n",
    "11111111111111100000011100000\n",
    "11111111111111100000011100000\n",
    "00000000000011100000011100000\n",
    "00000000000001100000011000000\n",
    "00000000000011100000011100000\n",
    "00000000000011100000011000000\n",
    "00000000000001100000011100000\n",
    "00000000000001100000011111111\n",
    "00000000000011100000011111111\n",
    "00000000000011100000011100000\n",
    "00000000000011100000011000000\n",
    "00000000000000000000011100000\n",
    "00000000000000000000011100000\n",
    "00000000000000000000011000000\n",
    "00000000000000000000011000000\n",
    "00000000000000000000011000000\n",
    "00000000000000000000011000000\n",
    "00000000000000000000011000000\n",
    "00000000000000000000011100000\n",
    "00000000000000000000011000000\n",
    "```\n",
    "\n",
    "### What We Need To Do\n",
    "We need to complete the following steps\n",
    "1. Write a file parser to turn each sample into a 2D numpy array\n",
    "2. Write a function to save all of the 2D numpy arrays as images \n",
    "3. Iterate through all the files and save the images to train/validation/test folders\n",
    "\n",
    "#### Parsing Files Into Numpy Arrays\n",
    "The following code will parse the file and turn each image into a Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np \n",
    "\n",
    "def load_images(file):\n",
    "    \"\"\"Return all the characters as a list of 2D numpy arrays\"\"\"\n",
    "    # Precompile match patterns for reuse \n",
    "    image_dimensions_regex = re.compile(r'^(\\d+) (\\d+)$')\n",
    "    image_binary_regex = re.compile(r'^(\\d+)$')\n",
    "\n",
    "    # Return all images from file \n",
    "    images = [] \n",
    "\n",
    "    # Open the file \n",
    "    with open(file, 'r') as file:\n",
    "        sample_id = 0 \n",
    "        image = None \n",
    "        for line in file: \n",
    "            # Blank Lines: Add image to list \n",
    "            if line == '\\n':\n",
    "                if image is not None:\n",
    "                    images.append(image)\n",
    "                    image = None\n",
    "                continue \n",
    "\n",
    "            # Sample IDs: Increment the sample number \n",
    "            if '_' in line:\n",
    "                sample_id += 1\n",
    "                continue \n",
    "\n",
    "            # Image Dimensions: Create numpy array \n",
    "            dims = re.match(image_dimensions_regex, line)\n",
    "            if dims:\n",
    "                rows = int(dims.group(1))\n",
    "                cols = int(dims.group(2))\n",
    "                row_index = iter(range(rows))\n",
    "                image = np.zeros((rows, cols))\n",
    "\n",
    "            # Binary Image: Add each row to array\n",
    "            row_data = re.match(image_binary_regex, line)\n",
    "            if row_data:\n",
    "                row = next(row_index)\n",
    "                data = [int(c) for c in list(row_data.group(1))]\n",
    "                image[row] = data\n",
    "\n",
    "    return images          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a list of images and an output directory, this code will save each image as a jpeg. Since our images were 0 or 1 before, we multiply them by 255. This means we will have to scale them when we load them into our convnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def save_images(images, output_dir):\n",
    "    \"\"\"Saves an array of numpy images to the specified output directory\"\"\"\n",
    "    for (idx, image) in enumerate(images):\n",
    "        img = Image.fromarray(image*255.).convert(\"L\")\n",
    "        img_name = str(idx) + '.jpg'\n",
    "        output_path = os.path.join(output_dir, img_name)\n",
    "        img.save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this script creates all the output directories, iterates through all the files, and saves the images accordingly. You should change the source directory and the target directory based on your information. Also, we use 1,187 images for training, 500 images for validation, and the remaining (500) images for testing. You can adjust this if you want as well. \n",
    "\n",
    "NOTE: This took 2 hours and 10 minutes to run on my MacBook Pro üëéü§ïüëé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# UPDATE THESE  \n",
    "# ------------------------------------------------------------\n",
    "phd08_source_dir = '/Users/jtbergman/Datasets/phd08/'\n",
    "phd08_target_dir = '/users/jtbergman/Datasets/phd08processed'\n",
    "\n",
    "# Paths for train, test, validation directories \n",
    "train_dir = os.path.join(phd08_target_dir, 'train')\n",
    "test_dir = os.path.join(phd08_target_dir, 'test')\n",
    "val_dir = os.path.join(phd08_target_dir, 'validation')\n",
    "\n",
    "# Train / Val / Test split\n",
    "train_split = 1187 \n",
    "val_split = 500\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# DON'T CHANGE   \n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def mkdir(directories):\n",
    "    \"\"\"Create directories if they don't already exist.\"\"\"\n",
    "    if type(directories) != list:\n",
    "        directories = [directories]\n",
    "    for d in directories:\n",
    "        if not os.path.exists(d):\n",
    "            os.mkdir(d)\n",
    "\n",
    "mkdir([phd08_target_dir, train_dir, test_dir, val_dir])\n",
    "\n",
    "def output_directories_for_file(file):\n",
    "    \"\"\"Return output directories for a character's images.\"\"\"\n",
    "    filename = os.path.basename(file)\n",
    "    character = filename.split('.')[0]\n",
    "    train_out = os.path.join(train_dir, character) \n",
    "    test_out = os.path.join(test_dir, character)\n",
    "    val_out = os.path.join(val_dir, character)\n",
    "    mkdir([train_out, test_out, val_out])\n",
    "    return train_out, test_out, val_out \n",
    "\n",
    "# Iterate over all the files and save them to train/dev/test\n",
    "for file in os.listdir(phd08_source_dir):\n",
    "    train, test, val = output_directories_for_file(file)\n",
    "    images = load_images(os.path.join(phd08_source_dir,file))\n",
    "    save_images(images[:train_split], train)\n",
    "    save_images(images[train_split:train_split+val_split], val)\n",
    "    save_images(images[train_split+val_split:], test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Architecture Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
