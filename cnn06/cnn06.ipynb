{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4EaJhjclU_MO"
   },
   "source": [
    "# Neural Style Transfer with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Vm4g6iSU_MQ"
   },
   "source": [
    "---\n",
    "## Introduction\n",
    "In this post, we are going to use Keras to implement neural style transfer. In neural style transfer, we take a content image and a style image and generate a new version of the content image based on the textures in the style image. For example, take a look at the following photo. \n",
    "\n",
    "![Example of neural style transfer](https://sunshineatnoon.github.io/assets/posts/2017-05-19-a-brief-summary-on-neural-style-transfer/1.png)\n",
    "\n",
    "Any pretrained convnet can be used to implement neural style transfer, but we will be basing our implementation on the [original paper](https://arxiv.org/pdf/1508.06576.pdf), so we will use the pretrained VGG19 network that is included with Keras. \n",
    "\n",
    "There are essentially three steps to the process\n",
    "1. First, we have to configure our network to compute layer activations for the content image, the style image, and the generated image simultaneously. We can do this by treating the three images as a single mini-batch. \n",
    "2. Second, we have to use the outputs of the layers to define a meaningful loss function so that when we minimize it, it creates an image with the content of our content image and the style of our style image. \n",
    "3. Finally, we just run gradient-descent and minimize the loss function by continuously updating the pixel values of our generated image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preparing our Data\n",
    "\n",
    "### Pratical Tip\n",
    "So far, all the projects have worked fine on a CPU. Neural style transfer, however, runs really, _really_ slow without a GPU. Running this on my MacBook Pro took approximately 525 seconds per iteration, but it only took 22 seconds per iteration using the free GPUs available through [Google Colaboratory](https://colab.research.google.com/). If you're unsure how to set this up, I have a post on it in the Appendix for this project. Once you've read that, be sure to run the code below to access local files in your Drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DVLn-P7ZV7Mp"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data \n",
    "Next, provide the paths to your target image and style image. You can see examples of the images I used in the `inputs` and `style` directories for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0cY0N4f1U_MS"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# CHANGE: Set this based on your images\n",
    "content_image_path = '/content/drive/My Drive/Colab Notebooks/cnn06/inputs/IU_portrait.png'\n",
    "style_image_path = '/content/drive/My Drive/Colab Notebooks/cnn06/style/the-starry-night.jpg'\n",
    "\n",
    "# Dimensins for the generated picture\n",
    "width, height = load_img(content_image_path).size\n",
    "img_height = 400\n",
    "img_width = int(width * img_height / height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UuI0xobsU_MX"
   },
   "source": [
    "### Transforming the Data\n",
    "As always, we have to do some preprocessing before we can feed our data into a neural network. In the case of VGG19, we have to use the `preprocess_input()` function, which you can se in the [Keras Applications Documentation](https://keras.io/applications/#usage-examples-for-image-classification-models). Furthermore, we have to undo some transformations that VGG19 performs, so we will write a `deprocess_image()` helper as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KwhqhcofU_MZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications import vgg19 \n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_height, img_width))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img \n",
    "\n",
    "def deprocess_image(image):\n",
    "    # Reverses a transformation in vgg19.preprocess_input()\n",
    "    image[:, :, 0] += 103.939\n",
    "    image[:, :, 1] += 116.779\n",
    "    image[:, :, 2] += 123.68 \n",
    "    # Convert from BGR to RGB (also vgg19.preprocess_input())\n",
    "    image = image[:, :, ::-1]\n",
    "    return np.clip(image, 0, 255).astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RBMln7I0U_Mf"
   },
   "source": [
    "### Loading VGG19 and Applying to Three Images\n",
    "Finally, we can load the VGG19 network just as we've done before. Previously, we've been configure our networks to work on random minibatchs of training data, but for neural style transfer we are always going to feed in the same three images. The target and style image are never going to change so we load them as constants. We use a placeholder to pass in the constantly changing generated image. In order to feed these all through the network at the same time, we concatenate them into a minibatch. \n",
    "\n",
    "As a final note, since our loss function is defined using the outputs of convolutional layers, we don't need to load the densely connected layers on top of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zP6XvNSSU_Mg"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K \n",
    "\n",
    "# Generated image is a placeholder others are constant \n",
    "content_image = K.constant(preprocess_image(content_image_path))\n",
    "style_image = K.constant(preprocess_image(style_image_path))\n",
    "generated_image = K.placeholder((1, img_height, img_width, 3))\n",
    "\n",
    "# Combine the images into a \"batch\" \n",
    "input_tensor = K.concatenate([content_image, style_image, generated_image], axis=0)\n",
    "\n",
    "# Load the model \n",
    "model = vgg19.VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9wh29YzsU_Ml"
   },
   "source": [
    "---\n",
    "## Defining the Loss Function \n",
    "The loss function for neural style transfer is a little weird, and it has three separate components: (1) the content loss, (2) the style loss, and (3) the variation loss. The content loss measures how well our image resembles the content image, the style loss measures how well our image resembles the style image, and the variation loss punishes our generated image for poor continuity – that is, it encourages smooth transitions in the pixels. We will assign weights to each of these loss functions then sum them together as our overall loss.\n",
    "\n",
    "### Content Loss \n",
    "Content loss is probably the simplest to understand, we can simply take the distance between the content image and the generated image using the sum of squared differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "v6mf_jFVU_Mn"
   },
   "outputs": [],
   "source": [
    "def content_loss(target, generated):\n",
    "    return K.sum(K.square(generated - target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Loss\n",
    "Style loss is a bit harder to understand. Here we have to take the sum of squared differences between the gram matrix of the style image and the gram matrix of the generated image. The gram matrix essentially measures the correlation between each pair of channels which is intended to tell us what textures should occur together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    \"\"\"Captures the 'style' of an image.\"\"\"\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram \n",
    "\n",
    "def style_loss(style, generated):\n",
    "    S = gram_matrix(style)\n",
    "    G = gram_matrix(generated)\n",
    "    channels = 3 \n",
    "    size = img_height * img_width\n",
    "    return K.sum(K.square(S - G)) / (4. * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Loss\n",
    "Finally, the variational loss looks at the continuity of pixel values in both the vertical and horizontal directions to ensure that the generated image is not becoming too pixelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    a = K.square(x[:, :img_height - 1, :img_width - 1, :] -\n",
    "                 x[:, 1:, :img_width - 1, :])\n",
    "    b = K.square(x[:, :img_height - 1, :img_width - 1, :] -\n",
    "                 x[:, :img_height - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing our Layers\n",
    "We need to combine the content loss, style loss, and variational loss into a single function, but first we have to decide which layer outputs to use for content and style. \n",
    "\n",
    "For content, we want to use a layer that captures a lot of high level features in the image to preserve as much of the original content as possible. For this reason, we are going to use a deep layer for content loss. In this case, we choose `block5_conv2` as our content layer.\n",
    "\n",
    "For style layers, we are going to combine the style layers from various depth to capture styles at different levels. For instance, capturing the edges, patterns, textures, and perhaps some higher level stylistic features as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary: layer name => layer output \n",
    "layers_dict = {layer.name: layer.output for layer in model.layers}\n",
    "\n",
    "# Our layer selections\n",
    "content_layer = 'block5_conv2'\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                'block5_conv1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZu6FPaYU_Ms"
   },
   "source": [
    "### The Loss Function\n",
    "Finally, we combine all of the loss functions together. It is up to use to determine how we want to weight the style, content, and variation losses in our final computation. These weights are hyperparemeters, so feel free to change them. I got pretty good results with the ones below, but I also have fine results with drastically different parameters. Note that the content loss is only between the content image and generated image, the style loss is only between the style image and generated image, and the variational loss only involves the generated image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NRDHQI8UU_Mu"
   },
   "outputs": [],
   "source": [
    "# Weights in the weighted average \n",
    "total_variation_weight = 1e-2 \n",
    "style_weight = 0.5 \n",
    "content_weight = 0.5\n",
    "\n",
    "# Overall Loss \n",
    "loss = K.variable(0.)\n",
    "\n",
    "# Loss += Content Loss \n",
    "layer_features = layers_dict[content_layer]\n",
    "content_image_featues = layer_features[0, :, :, :]\n",
    "generated_image_features = layer_features[2, :, :, :]\n",
    "loss = loss + content_weight * content_loss(content_image_featues,\n",
    "                                            generated_image_features)\n",
    "\n",
    "# Loss += Style Loss \n",
    "for layer_name in style_layers: \n",
    "    layer_features = layers_dict[layer_name]\n",
    "    style_image_features = layer_features[1, :, :, :]\n",
    "    generated_image_features = layer_features[2, :, :, :]\n",
    "    temp_style_loss = style_loss(style_image_features, \n",
    "                                 generated_image_features)\n",
    "    loss = loss + (style_weight / len(style_layers)) * temp_style_loss\n",
    "    \n",
    "# Loss += Variation Loss \n",
    "loss = loss + total_variation_weight * total_variation_loss(generated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZ72pTSxU_Mz"
   },
   "source": [
    "---\n",
    "## Gradient-Descent \n",
    "As we've done before, we use the Keras Backend to get the gradients of the loss function with respect to the generated image. Then, we define a function that can provide the current loss and gradients for the generated image. \n",
    "\n",
    "One tricky thing is the optimization... In the original paper, the L-BFGS optimization algorithm is used. We will use it too, and you can read about it on [Wikipedia](https://en.wikipedia.org/wiki/Limited-memory_BFGS), but fortunately we don't have to implement it ourselves! Unfortunately, it isn't provided in Keras, so we have to use Scipy. Furthermore, the Scipy requires two separate functions to retrieve the loss and gradients. We have a single function `fetch_loss_and_grads()` for this. To avoid duplicated computations, we can use this evaluator class to store the result and only make one function call. Eventually I may change this, but for now let's stick with the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HrVX8Wa1U_M0"
   },
   "outputs": [],
   "source": [
    "grads = K.gradients(loss, generated_image)[0]\n",
    "fetch_loss_and_grads = K.function([generated_image], [loss, grads])\n",
    "\n",
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        x = x.reshape((1, img_height, img_width, 3))\n",
    "        outs = fetch_loss_and_grads([x])\n",
    "        loss_value = outs[0]\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5tigqZzU_M6"
   },
   "source": [
    "---\n",
    "## Neural Style Transfer\n",
    "Now, all we have to do is specify the number of desired iterations, provide our initial image, and continuosly perform gradient descent on our loss function. Each iteration actually performs 20 steps of gradient descent (`maxfun=20`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1037
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 259799,
     "status": "ok",
     "timestamp": 1544960409291,
     "user": {
      "displayName": "JT Bergman",
      "photoUrl": "https://lh6.googleusercontent.com/-f2QnRTfJH_M/AAAAAAAAAAI/AAAAAAAAAfo/ObyOWArlkVQ/s64/photo.jpg",
      "userId": "17974959315589527400"
     },
     "user_tz": -540
    },
    "id": "cHMORPzVU_M8",
    "outputId": "2087e77b-6ca6-4747-8d1f-f5ba3491d88e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of iteration 1\n",
      "Current loss value: 1192790300.0\n",
      "Start of iteration 2\n",
      "Current loss value: 576547700.0\n",
      "Start of iteration 3\n",
      "Current loss value: 413295300.0\n",
      "Start of iteration 4\n",
      "Current loss value: 337355140.0\n",
      "Start of iteration 5\n",
      "Current loss value: 294651900.0\n",
      "Start of iteration 6\n",
      "Current loss value: 264794980.0\n",
      "Start of iteration 7\n",
      "Current loss value: 238312660.0\n",
      "Start of iteration 8\n",
      "Current loss value: 222517490.0\n",
      "Start of iteration 9\n",
      "Current loss value: 208912460.0\n",
      "Start of iteration 10\n",
      "Current loss value: 193770380.0\n",
      "Start of iteration 11\n",
      "Current loss value: 184535520.0\n",
      "Start of iteration 12\n",
      "Current loss value: 178183970.0\n",
      "Start of iteration 13\n",
      "Current loss value: 171971380.0\n",
      "Start of iteration 14\n",
      "Current loss value: 166173570.0\n",
      "Start of iteration 15\n",
      "Current loss value: 162028960.0\n",
      "Start of iteration 16\n",
      "Current loss value: 158924140.0\n",
      "Start of iteration 17\n",
      "Current loss value: 155823730.0\n",
      "Start of iteration 18\n",
      "Current loss value: 153069360.0\n",
      "Start of iteration 19\n",
      "Current loss value: 150175220.0\n",
      "Start of iteration 20\n",
      "Current loss value: 147637300.0\n",
      "Start of iteration 21\n",
      "Current loss value: 145015890.0\n",
      "Start of iteration 22\n",
      "Current loss value: 142706100.0\n",
      "Start of iteration 23\n",
      "Current loss value: 140533580.0\n",
      "Start of iteration 24\n",
      "Current loss value: 138491380.0\n",
      "Start of iteration 25\n",
      "Current loss value: 136699800.0\n",
      "Start of iteration 26\n",
      "Current loss value: 135008320.0\n",
      "Start of iteration 27\n",
      "Current loss value: 133602060.0\n",
      "Start of iteration 28\n",
      "Current loss value: 132335416.0\n",
      "Start of iteration 29\n",
      "Current loss value: 130920300.0\n",
      "Start of iteration 30\n",
      "Current loss value: 129522990.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import imageio\n",
    "\n",
    "# Provide your output path and intended iterations \n",
    "base_path = '/content/drive/My Drive/Colab Notebooks/cnn06/outputs/'\n",
    "iterations = 30\n",
    "\n",
    "# Our generated image is initially the content image\n",
    "x = preprocess_image(content_image_path)\n",
    "x = x.flatten()\n",
    "\n",
    "# Save the first image \n",
    "original_path = os.path.join(base_path, '0.png')\n",
    "original_img = deprocess_image(x.reshape((img_height, img_width, 3)))\n",
    "imageio.imwrite(original_path, original_img)\n",
    "\n",
    "# Perform the intended number of iterations\n",
    "for i in range(1, iterations + 1):\n",
    "    print('Start of iteration', i)\n",
    "    x, min_val, _ = fmin_l_bfgs_b(evaluator.loss, x, fprime=evaluator.grads, maxfun=20)\n",
    "    print('Current loss value:', min_val)\n",
    "    img = x.copy().reshape((img_height, img_width, 3))\n",
    "    img = deprocess_image(img)\n",
    "    fname = os.join.path(base_path, str(i) + '.png')\n",
    "    imageio.imwrite(fname, img)\n",
    "\n",
    "# Save the final output\n",
    "final_path = os.join.path(base_path, 'final.png')\n",
    "imageio.imwrite(final_path, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating GIFs from Outputs \n",
    "Before showing you some of my results, here is a helper function you can use to transform a directory of numerically ordered photos (1.png, 2.png, 3.jpg, etc.) into a GIF. This allows us to see the photo transforming over time, which is pretty cool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "n6H1icTKU_NF"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import re \n",
    "import imageio\n",
    "\n",
    "# only keep files that followed the `number.png` convention\n",
    "file_format = re.compile(r'(\\d+).png')\n",
    "\n",
    "# Sort the images numerically (e.g. 1.png, 2.png, ...)\n",
    "def sort_key(filename):\n",
    "    \"\"\"Return the numeric portion of filename.\"\"\"\n",
    "    return int(filename.split('.')[0])\n",
    "\n",
    "# Transform the files in the directory into a gif \n",
    "def images_to_gif(source_dir, name='final.gif'):\n",
    "    \"\"\"Turn a directory of numerically labeled photos into gifs.\"\"\"\n",
    "    files = sorted([fname for fname in os.listdir(source_dir) \n",
    "                    if file_format.match(fname)], key=sort_key)\n",
    "    images = [imageio.imread(os.path.join(source_dir, fname)) \n",
    "              for fname in files]\n",
    "    imageio.mimsave(os.path.join(source_dir, name), images, \n",
    "                    'GIF', duration=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used this to generate an image for all of my photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '/content/drive/My Drive/Colab Notebooks/cnn06/outputs/img7/'\n",
    "images_to_gif(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results\n",
    "\n",
    "### Example 1\n",
    "```python \n",
    "# Images\n",
    "content_image = './inputs/germany.jpg'\n",
    "style_image = './style/the_shipwreck_of_the_minotaur.jpg'\n",
    "\n",
    "# Parameters\n",
    "total_variation_weight = 1e-4\n",
    "style_weight = 1.\n",
    "content_weight = 0.025\n",
    "```\n",
    "\n",
    "![Neckarfront in Tübingeng, Germany](./outputs/img1/final.gif)\n",
    "\n",
    "### Example 2\n",
    "```python \n",
    "# Images\n",
    "content_image = './inputs/IU_portrait.png'\n",
    "style_image = './style/femme_nue_assise_pablo_picasso.jpg'\n",
    "\n",
    "# Parameters\n",
    "total_variation_weight = 1e-4\n",
    "style_weight = 1.\n",
    "content_weight = 0.025\n",
    "```\n",
    "\n",
    "![IU + Femme Nue Assise](./outputs/img2/final.gif)\n",
    "\n",
    "### Example 3\n",
    "```python \n",
    "# Images\n",
    "content_image = './inputs/IU_portrait.png'\n",
    "style_image = './style/composition_vii.jpg'\n",
    "\n",
    "# Parameters\n",
    "total_variation_weight = 1e-2\n",
    "style_weight = 0.5\n",
    "content_weight = 0.5\n",
    "```\n",
    "\n",
    "![IU + composition vii](./outputs/img3/final.gif)\n",
    "\n",
    "### Example 4\n",
    "```python \n",
    "# Images\n",
    "content_image = './inputs/IU_portrait.png'\n",
    "style_image = './style/the-starry-night.jpg'\n",
    "\n",
    "# Parameters\n",
    "total_variation_weight = 1e-2\n",
    "style_weight = 0.5\n",
    "content_weight = 0.5\n",
    "```\n",
    "\n",
    "![IU + Starry Night](./outputs/img4/final.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary \n",
    "In my opinion, this is a lot cooler than DeepDream because we have way more influence over what the final image look likes. I actually think it would be better to actual slow the learning down so that we can create GIFs with much smoother transitions. If you look a the outputs,by just the third iteration the generated image is usually a lot closer to the final output than the content image. \n",
    "\n",
    "### Original Neural Style Transfer Paper\n",
    "You can find the original neural style transfer paper [here](https://arxiv.org/pdf/1508.06576.pdf)\n",
    "\n",
    "### Content Images \n",
    "The first content image is of the Neckarfront in Tübingen, Germany. This is one of the images used in the original paper. The second content image is a portrait of IU – a Korean singer-songwriter and actress. If you're interested... [IU - 삐삐](https://youtu.be/nM0xDI5R50E).\n",
    "\n",
    "### Style Images \n",
    "All of the style images were used in the original NST paper as well. \n",
    "+ _The Shipwreck of the Minotaur_ by JMW Turner (1805)\n",
    "+ _Femme nue assise_ by Pablo Picasso (1910)\n",
    "+ _Composition VII_ by Wassily Kandinsky (1913)\n",
    "+ _The Starry Night_ by Vincent van Gogh (1889)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "cnn06.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
